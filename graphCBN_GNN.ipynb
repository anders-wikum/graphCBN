{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from tqdm import trange\n",
    "import copy\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Graph generation\n",
    "Here, we generate many graphs in the DIMACS minimum cost flow structure. Any minimum cost flow problem in this representation can be used for training.\n",
    "More specifics on the file representation: [DIMACS minimum cost flow problem structure](https://lpsolve.sourceforge.net/5.5/DIMACS_mcf.htm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 101 min cost flow graphs\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from graph_generator import generate_min_cost_flow_graphs\n",
    "\n",
    "generate_min_cost_flow_graphs(num = 101, dst_folder = \"./data/raw\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch geometric dataset\n",
    "The MinCostDataset class is a custom pytorch geometric dataset class that allows transforms graphs from the DIMACS problem structure into the graph format expected by PyG."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MinCostDataset import MinCostDataset\n",
    "\n",
    "dataset = MinCostDataset(root = \"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of graphs: 101\n",
      "number of node features: 1\n",
      "number of edge features: 2\n",
      "first graph: Data(x=[102, 1], edge_index=[2, 255], edge_attr=[255, 2], y=[102, 2], reduced_cost=[255, 2], filename='netgen_72.txt')\n"
     ]
    }
   ],
   "source": [
    "def dataset_information(dataset):\n",
    "    \"\"\"Print some basic information about the dataset\"\"\"\n",
    "    print(f\"number of graphs: {dataset.len()}\")\n",
    "    print(f\"number of node features: {dataset.num_features}\")\n",
    "    print(f\"number of edge features: {dataset.num_edge_features}\")\n",
    "    print(f\"first graph: {dataset[0]}\")\n",
    "\n",
    "\n",
    "dataset_information(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset split\n",
    "Since we are not working with a predefined dataset, we have to manually create training, testing, and validation splits. Since some training tasks we test are at the graph-level (for example, learning optimal duals from the dual value), we split at the level of graphs instead of nodes/edges."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_validation_split(dataset, train = 0.7, validation = 0.15):\n",
    "    \"\"\"\n",
    "    Splits a dataset into a training, validation, and test split. The test fraction is 1 - train - validation.\n",
    "    Similar to sklearn.model_selection.train_test_split with the addition to a validation split.\n",
    "    Args:\n",
    "        dataset: dataset to split\n",
    "        train: fraction of the dataset to allocate to training\n",
    "        validation: fraction of the dataset to allocate to validation\n",
    "\n",
    "    Returns:\n",
    "        An index for each train/validation/test datasets.\n",
    "    \"\"\"\n",
    "    length = dataset.len()\n",
    "    shuffled_dataset = np.arange(length)\n",
    "    np.random.shuffle(shuffled_dataset)\n",
    "\n",
    "    train_cutoff = int(train * length)\n",
    "    validation_cutoff = int((train + validation) * length)\n",
    "\n",
    "    train_data = shuffled_dataset[:train_cutoff]\n",
    "    validation_data = shuffled_dataset[train_cutoff: validation_cutoff]\n",
    "    test_data = shuffled_dataset[validation_cutoff:]\n",
    "\n",
    "    return train_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_split_directories(dataset, split, split_name):\n",
    "    \"\"\"\n",
    "    Creates a new directory for the dataset and saves each corresponding file into it.\n",
    "    Args:\n",
    "        dataset: dataset from which to copy the files\n",
    "        split: indices of the files to move to the new directory\n",
    "        split_name: name of the new directory\n",
    "    \"\"\"\n",
    "\n",
    "    src_folder = dataset.processed_dir\n",
    "    dst_folder = os.path.join(dataset.root, split_name)\n",
    "\n",
    "    # Remove files in case some were already present\n",
    "    if os.path.exists(dst_folder):\n",
    "        shutil.rmtree(dst_folder)\n",
    "    os.makedirs(dst_folder)\n",
    "\n",
    "    # Files are always expected by PyG to be ordered from 0 to length of the dataset so we have to\n",
    "    # reorder them in the new directory\n",
    "    dst_index = 0\n",
    "    for file_id in split:\n",
    "        src_file_name = f\"data_{file_id}.pt\"\n",
    "        dst_file_name = f\"data_{dst_index}.pt\"\n",
    "        src = os.path.join(src_folder, src_file_name)\n",
    "        dst = os.path.join(dst_folder, dst_file_name)\n",
    "        shutil.copyfile(src, dst)\n",
    "        dst_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_frac = 0.7, validation_frac = 0.15):\n",
    "    \"\"\"\n",
    "    Partitions a dataset into 3 folders: data_train, data_test, data_validation such that each contains\n",
    "    a certain fraction of the graphs.\n",
    "    Args:\n",
    "        dataset: dataset to split\n",
    "        train_frac: fraction of the graphs to put into the training folder\n",
    "        validation_frac: fraction of the graphs ot put into the validation folder\n",
    "    Note: test_frac is implicitly defined as 1 - train_frac - validation frac\n",
    "    \"\"\"\n",
    "\n",
    "    train, validation, test = train_test_validation_split(dataset, train_frac, validation_frac)\n",
    "    create_split_directories(dataset, train, \"data_train/processed\")\n",
    "    create_split_directories(dataset, test, \"data_test/processed\")\n",
    "    create_split_directories(dataset, validation, \"data_validation/processed\")\n",
    "\n",
    "\n",
    "split_dataset(dataset, train_frac = 0.7, validation_frac = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss functions\n",
    "We define the different loss functions we have experimented for learning duals. This first one (DualLoss) pools over all predicted node potentials (a scalar value predicted for each node) then computes the objective value of the dual in the graph. The corresponding formula comes directly from the dual expression of min cost flow:\n",
    "$$\\widehat{\\text{OPT}} = \\sum_{i \\in V} b_i\\hat{y}_i - \\sum_{(i,j) \\in E} u_{ij} \\cdot \\max(0, \\hat{y}_j - \\hat{y}_i - c_{ij})$$\n",
    "Loss is then computed by:\n",
    "$$\\text{LOSS} (\\text{OPT}, \\widehat{\\text{OPT}}) = \\text{OPT} - \\widehat{\\text{OPT}}$$\n",
    "Notice that we don't add a norm on the substraction since, by weak duality $\\text{LOSS}(\\text{OPT}, \\widehat{\\text{OPT}}) \\geq 0$ with $\\text{LOSS}(\\text{OPT}, \\widehat{\\text{OPT}}) = 0$ if and only if $\\hat{y}$ is a dual-optimal solution.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    This loss function computes the objective value of the learned duals following the above formula with an optional regularization factor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DualLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, label, x, edge_index, edge_attr, gamma = 0.0005):\n",
    "        \"\"\"\n",
    "        Computes the dual loss\n",
    "        Args:\n",
    "            pred: predicted node potentials\n",
    "            label: optimal dual value in the graphs\n",
    "            x: original node features (we need to know the demand of each node to compute the loss)\n",
    "            edge_index: edge structure of the graph\n",
    "            edge_attr: original edge features (we need to know the cost and capacity of each edge to compute the loss)\n",
    "            gamma: regularization factor to push learned potentials towards more 0 reduced cost edges\n",
    "\n",
    "        Returns:\n",
    "            the corresponding loss value\n",
    "        \"\"\"\n",
    "        reduced_cost = edge_attr[:, 1] + pred[edge_index[0]].squeeze() - pred[edge_index[1]].squeeze()\n",
    "        opt = label[0, 0]  # Optimal dual value in the graph\n",
    "\n",
    "        # PyTorch retranscription of the above equation, notice that the loss is divided by the optimal dual value\n",
    "        # to make it independent of graph size\n",
    "        loss = (opt + torch.dot(pred.squeeze(), x.squeeze()) + torch.dot(edge_attr[:, 0],\n",
    "                                                                             F.relu(-reduced_cost))) / opt\n",
    "\n",
    "        reg = gamma * sum(F.relu(-reduced_cost))\n",
    "\n",
    "        return loss + reg"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Edge reduced cost loss\n",
    "Instead of trying to predict node potential values, here we try to predict if a given edge will have negative reduced cost or positive reduced cost (this information can then used to \"warm-start\" the algorithm). In this case, we are essentially doing edge classification so the loss definition is much simpler, simply computing a weighted cross entropy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class EdgeReducedCostLoss(nn.Module):\n",
    "    def __init__(self, weight = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weight: weighs the different classes differently, useful for an unbalanced dataset (which is the case here)\n",
    "        \"\"\"\n",
    "        super(EdgeReducedCostLoss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight = weight)\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        return self.loss_fn(preds, labels)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Potential loss\n",
    "This loss function is most similar to the first one (DualLoss). However, here, instead of learning duals such that their objective value is close to optimal, we learn dual values that are close to a set of optimal dual values. This transforms the task from a graph-level regression to a node-level regression.\n",
    "\n",
    "We note that this approach may seem theoretically unsound, there are indeed several possible sets of optimal dual values possible, all attaining optimal objective value so learning exactly one set seems illogical. However, empirically, this method is the one that works best. From experiments, it seems to be because DualLoss had a much higher noise to signal ratio. This makes sense intuitively, here we predict one value per node whereas in DualLoss, we essentially predict one value for the entire graph then expect to backpropagate this single scalar value to the thousands of parameters in the GNN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class PotentialLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplest loss: it just returns the L1 difference between a set of optimal dual values (that is, a scalar value per node in the graph)\n",
    "    and the computed duals\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PotentialLoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        return torch.linalg.vector_norm(preds.squeeze() - labels, ord = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    For edge-level classification, computes the accuracy of the predictions\n",
    "    Args:\n",
    "        preds: predicted edge classes\n",
    "        labels: ground truth edge classes\n",
    "    \"\"\"\n",
    "    predicted_classes = torch.argmax(preds, dim = 1)\n",
    "    label_values = torch.argmax(labels, dim = 1)\n",
    "\n",
    "    # True positives for edges with negative reduced cost\n",
    "    count1 = predicted_classes[predicted_classes == label_values == 0]\n",
    "    accuracy1 = predicted_classes.sum() / label_values[label_values == 0].sum()\n",
    "\n",
    "    # True positives for edges with positive (>= 0) reduced cost\n",
    "    count2 = predicted_classes[predicted_classes == label_values == 1]\n",
    "    accuracy2 = predicted_classes.sum() / label_values[label_values == 1].sum()\n",
    "\n",
    "    # Prints the percentage of\n",
    "    print(f\"predicted % for class 0/neg: {accuracy1 * 100}\")\n",
    "    print(f\"predicted % for class 1/pos: {accuracy2 * 100}\")\n",
    "\n",
    "    accuracy = torch.mean(torch.where(predicted_classes == label_values, 1, 0).float()).item()\n",
    "\n",
    "    return round(accuracy, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    \"\"\"\n",
    "    Builds an optimizer according to the given parameters. From Colab 2:\n",
    "    https://colab.research.google.com/drive/1xHmpjVO-Z74NK-dH3qoUBTf-tKUPfOKW?usp=sharing\n",
    "    \"\"\"\n",
    "\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p: p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr = args.lr, momentum = 0.95, weight_decay = weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr = args.lr, weight_decay = weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = args.opt_decay_step, gamma = args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "# args defines the model and training hyperparameters\n",
    "args = {\n",
    "    'num_layers':    3,\n",
    "    'batch_size':    64,\n",
    "    'hidden_dim':    32,\n",
    "    'heads':         4,\n",
    "    'predict_edges': False,\n",
    "    'dropout':       0.5,\n",
    "    'epochs':        100,\n",
    "    'opt':           'adam',\n",
    "    'opt_scheduler': 'none',\n",
    "    'opt_restart':   0,\n",
    "    'weight_decay':  5e-3,\n",
    "    'lr':            0.001,\n",
    "    'model_type':    \"GAT\"\n",
    "}\n",
    "args = objectview(args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from gnn_models.CBN_NNConv import CBN_NNConv\n",
    "from gnn_models.CBN_GAT import CBN_GAT\n",
    "\n",
    "# To make batching easier, we define 2 data loaders, one for training, one for testing\n",
    "train_loader = DataLoader(MinCostDataset(root = \"./data/data_train\"), batch_size = args.batch_size, shuffle = True)\n",
    "test_loader = DataLoader(MinCostDataset(root = \"./data/data_test\"), batch_size = args.batch_size, shuffle = True)\n",
    "\n",
    "def train(args):\n",
    "    \"\"\"\n",
    "    Trains a GNN model, periodically testing it and accumulating loss values\n",
    "    Args:\n",
    "        args: dictionary object containing training parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Input dimension is 1 (we only have demand information for every node)\n",
    "    # Edge feature dimension is 2 (capacity and cost per edge)\n",
    "    # Output dimension is 1 since we predict scalar potential values for each vertex\n",
    "    if args.model_type == \"GAT\":\n",
    "        model = CBN_GAT(1, 1, 2, args)\n",
    "    else:\n",
    "        model = CBN_NNConv(1, 1, 2, args)\n",
    "\n",
    "    if args.predict_edges:\n",
    "        loss_fn = EdgeReducedCostLoss()\n",
    "    else:\n",
    "        loss_fn = PotentialLoss()\n",
    "\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # accumulate model performance for plotting\n",
    "    losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    test_losses = []\n",
    "    best_loss = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in trange(args.epochs, desc = \"Training\", unit = \"Epochs\"):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            if args.predict_edges:\n",
    "                # For case of edge prediction, we classify edges according to their reduced cost\n",
    "                loss = loss_fn(pred, batch.reduced_cost)\n",
    "            else:\n",
    "                # For node potential prediction, we perform regression onto optimal potential values\n",
    "                # (contained in batch.y[:, 1]\n",
    "                loss = loss_fn(pred, batch.y[:, 1])\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if args.predict_edges:\n",
    "                # Accuracy only is computed for the classification task\n",
    "                total_accuracy += model.accuracy(pred, batch.reduced_cost) * batch.num_graphs\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        total_accuracy /= len(train_loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "        train_accs.append(total_accuracy)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_acc, test_loss = test(test_loader, model, loss_fn, args)\n",
    "            test_accs.append(test_acc)\n",
    "            test_losses.append(test_loss)\n",
    "            if best_loss is None or test_loss < best_loss:\n",
    "                best_loss = test_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "            test_accs.append(test_accs[-1])\n",
    "            test_losses.append(test_losses[-1])\n",
    "\n",
    "    return test_accs, test_losses, losses, train_accs, best_model, best_loss\n",
    "\n",
    "\n",
    "def test(loader, test_model, loss_fn, args):\n",
    "    test_model.eval()\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        with torch.no_grad():\n",
    "            pred = test_model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "\n",
    "            if args.predict_edges:\n",
    "                # For case of edge prediction, we classify edges according to their reduced cost\n",
    "                loss = loss_fn(pred, batch.reduced_cost)\n",
    "            else:\n",
    "                # For node potential prediction, we perform regression onto optimal potential values\n",
    "                # (contained in batch.y[:, 1]\n",
    "                loss = loss_fn(pred, batch.y[:, 1])\n",
    "\n",
    "            if args.predict_edges:\n",
    "                total_accuracy += test_model.accuracy(pred, batch.reduced_cost) * batch.num_graphs\n",
    "\n",
    "            total_loss += loss * batch.num_graphs\n",
    "\n",
    "\n",
    "    total_accuracy /= len(test_loader.dataset)\n",
    "    total_loss /= len(test_loader.dataset)\n",
    "\n",
    "    return total_accuracy, total_loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [00:27<00:00,  3.68Epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum test set accuracy: 0.0\n",
      "Maximum training set accuracy: 0.0\n",
      "Minimum loss: 59772.025362723216\n",
      "Minimum test loss: 16347.6064453125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_accs, test_losses, losses, train_accs, best_model, best_acc = train(args)\n",
    "\n",
    "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Maximum training set accuracy: {0}\".format(max(train_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "print(\"Minimum test loss: {0}\".format(min(test_losses)))\n",
    "\n",
    "# Run test for our best model to save the predictions!\n",
    "# test(test_loader, best_model, is_validation=False, save_model_preds=True, model_type=model)\n",
    "print()\n",
    "if args.predict_edges:\n",
    "    # If performing edge classification, we also plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(losses, label = \"training loss\" + \" - \" + args.model_type, color = 'red', linestyle = 'solid')\n",
    "plt.plot(test_losses, label = \"test loss\" + \" - \" + args.model_type, color = 'green', linestyle = 'solid')\n",
    "plt.legend()\n",
    "\n",
    "if args.predict_edges:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label = \"training accuracy\" + \" - \" + args.model_type, color = 'red', linestyle = 'dashed')\n",
    "    plt.plot(test_accs, label = \"test accuracy\" + \" - \" + args.model_type, color = 'green', linestyle = 'dashed')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig iters: 26, trained iters: 26, speedup: 0.0\n",
      "orig excess: 500, trained_excess: 452\n",
      "orig iters: 17, trained iters: 17, speedup: 0.0\n",
      "orig excess: 500, trained_excess: 482\n",
      "orig iters: 15, trained iters: 14, speedup: 6.666666666666667\n",
      "orig excess: 500, trained_excess: 456\n",
      "orig iters: 25, trained iters: 23, speedup: 8.0\n",
      "orig excess: 500, trained_excess: 482\n",
      "orig iters: 22, trained iters: 22, speedup: 0.0\n",
      "orig excess: 500, trained_excess: 424\n",
      "orig iters: 12, trained iters: 12, speedup: 0.0\n",
      "orig excess: 500, trained_excess: 474\n",
      "orig iters: 33, trained iters: 30, speedup: 9.090909090909092\n",
      "orig excess: 500, trained_excess: 444\n",
      "orig iters: 42, trained iters: 35, speedup: 16.666666666666664\n",
      "orig excess: 500, trained_excess: 432\n",
      "orig iters: 29, trained iters: 28, speedup: 3.4482758620689653\n",
      "orig excess: 500, trained_excess: 418\n",
      "orig iters: 26, trained iters: 26, speedup: 0.0\n",
      "orig excess: 500, trained_excess: 426\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dimensions (0,).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compute_dataset_metrics\n\u001B[1;32m      3\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m MinCostDataset(root \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data/data_test\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mcompute_dataset_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Stanford/Winter_2023/CS224W/Final project/temp/graphCBN/metrics.py:27\u001B[0m, in \u001B[0;36mcompute_dataset_metrics\u001B[0;34m(dataset, model, raw_dir)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dataset:\n\u001B[1;32m     26\u001B[0m     preds \u001B[38;5;241m=\u001B[39m model(data\u001B[38;5;241m.\u001B[39mx, data\u001B[38;5;241m.\u001B[39medge_index, data\u001B[38;5;241m.\u001B[39medge_attr)\n\u001B[0;32m---> 27\u001B[0m     preds \u001B[38;5;241m=\u001B[39m refine_preds(data, preds)\n\u001B[1;32m     28\u001B[0m     excess_diff, speedup \u001B[38;5;241m=\u001B[39m compute_graph_metrics(data, preds, raw_dir)\n\u001B[1;32m     29\u001B[0m     speedups\u001B[38;5;241m.\u001B[39mappend(speedup)\n",
      "File \u001B[0;32m~/Stanford/Winter_2023/CS224W/Final project/temp/graphCBN/metrics.py:91\u001B[0m, in \u001B[0;36mrefine_preds\u001B[0;34m(data, preds)\u001B[0m\n\u001B[1;32m     89\u001B[0m N \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mx\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     90\u001B[0m y \u001B[38;5;241m=\u001B[39m cp\u001B[38;5;241m.\u001B[39mVariable(N)\n\u001B[0;32m---> 91\u001B[0m t \u001B[38;5;241m=\u001B[39m \u001B[43mcp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mVariable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mneg_edges\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m s \u001B[38;5;241m=\u001B[39m cp\u001B[38;5;241m.\u001B[39mVariable(\u001B[38;5;28mlen\u001B[39m(pos_edges))\n\u001B[1;32m     94\u001B[0m prob \u001B[38;5;241m=\u001B[39m cp\u001B[38;5;241m.\u001B[39mProblem(\n\u001B[1;32m     95\u001B[0m     cp\u001B[38;5;241m.\u001B[39mMinimize(\u001B[38;5;241m10000\u001B[39m \u001B[38;5;241m*\u001B[39m cp\u001B[38;5;241m.\u001B[39msum(\u001B[38;5;241m-\u001B[39mcp\u001B[38;5;241m.\u001B[39mminimum(s, \u001B[38;5;241m0\u001B[39m)) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m*\u001B[39m cp\u001B[38;5;241m.\u001B[39mnorm(s\u001B[38;5;241m.\u001B[39mT \u001B[38;5;241m@\u001B[39m caps, \u001B[38;5;241m1\u001B[39m)),\n\u001B[1;32m     96\u001B[0m     [\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    100\u001B[0m     ]\n\u001B[1;32m    101\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/cvxpy/expressions/variable.py:86\u001B[0m, in \u001B[0;36mVariable.__init__\u001B[0;34m(self, shape, name, var_id, **kwargs)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mVariable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/cvxpy/expressions/leaf.py:108\u001B[0m, in \u001B[0;36mLeaf.__init__\u001B[0;34m(self, shape, value, nonneg, nonpos, complex, imag, symmetric, diag, PSD, NSD, hermitian, boolean, integer, sparsity, pos, neg)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m shape:\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(d, numbers\u001B[38;5;241m.\u001B[39mIntegral) \u001B[38;5;129;01mor\u001B[39;00m d \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 108\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid dimensions \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (shape,))\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(np\u001B[38;5;241m.\u001B[39mint32(d) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m shape)\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (PSD \u001B[38;5;129;01mor\u001B[39;00m NSD \u001B[38;5;129;01mor\u001B[39;00m symmetric \u001B[38;5;129;01mor\u001B[39;00m diag \u001B[38;5;129;01mor\u001B[39;00m hermitian) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mlen\u001B[39m(shape) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    112\u001B[0m                                                        \u001B[38;5;129;01mor\u001B[39;00m shape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m shape[\u001B[38;5;241m1\u001B[39m]):\n",
      "\u001B[0;31mValueError\u001B[0m: Invalid dimensions (0,)."
     ]
    }
   ],
   "source": [
    "from metrics import compute_dataset_metrics\n",
    "\n",
    "test_dataset = MinCostDataset(root = \"./data/data_test\")\n",
    "compute_dataset_metrics(test_dataset, best_model, dataset.raw_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
