{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/miniconda3/envs/cs224w/lib/python3.10/site-packages/nbformat/__init__.py:92: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of edges: 24\n",
      "Number of flow updates: 5, final flow value: 150\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/tmp/ipykernel_77539/1483371613.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m nodes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m]\n\u001B[1;32m      6\u001B[0m N \u001B[38;5;241m=\u001B[39m Network(nodes, edges, capacities, costs, supplies)     \n\u001B[0;32m----> 8\u001B[0m f, p \u001B[38;5;241m=\u001B[39m successive_shortest_paths(N)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrun\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msuccessive_shortest_paths.ipynb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2369\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2367\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[1;32m   2368\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m-> 2369\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2370\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/IPython/core/magics/execution.py:717\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[1;32m    715\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m preserve_keys(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39muser_ns, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__file__\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    716\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39muser_ns[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__file__\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m filename\n\u001B[0;32m--> 717\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msafe_execfile_ipy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraise_exceptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    718\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    720\u001B[0m \u001B[38;5;66;03m# Control the response to exit() calls made by the script being run\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2875\u001B[0m, in \u001B[0;36mInteractiveShell.safe_execfile_ipy\u001B[0;34m(self, fname, shell_futures, raise_exceptions)\u001B[0m\n\u001B[1;32m   2873\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_cell(cell, silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, shell_futures\u001B[38;5;241m=\u001B[39mshell_futures)\n\u001B[1;32m   2874\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m raise_exceptions:\n\u001B[0;32m-> 2875\u001B[0m     \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2876\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result\u001B[38;5;241m.\u001B[39msuccess:\n\u001B[1;32m   2877\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/IPython/core/interactiveshell.py:266\u001B[0m, in \u001B[0;36mExecutionResult.raise_error\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_before_exec\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_in_exec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 266\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_in_exec\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m/tmp/ipykernel_77539/1483371613.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m nodes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m]\n\u001B[1;32m      6\u001B[0m N \u001B[38;5;241m=\u001B[39m Network(nodes, edges, capacities, costs, supplies)     \n\u001B[0;32m----> 8\u001B[0m f, p \u001B[38;5;241m=\u001B[39m successive_shortest_paths(N)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "%run successive_shortest_paths.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def parse(filename) -> Network:\n",
    "    \"\"\"\n",
    "    Parses a network file following the DIMACS problem specification\n",
    "    structure and transforms it into a Network object\n",
    "\n",
    "    Some elements of the specification:\n",
    "    - Lines starting in c are comments\n",
    "    - Lines starting in p explain what problem to solve (can be ignored,\n",
    "      we only consider minimum-cost flow problems)\n",
    "    - Lines starting in n define nodes\n",
    "    - Lines starting in a define arcs (edges)\n",
    "\n",
    "    Args:\n",
    "        filename: name of the file containing the network data\n",
    "\n",
    "    Returns:\n",
    "        The corresponding Network object\n",
    "    \"\"\"\n",
    "    # Lines we can ignore\n",
    "    ignore_list = ['c', 'p']\n",
    "\n",
    "    file = open(filename, 'r')\n",
    "\n",
    "    # Nodes is a hashmap from node values to their supply\n",
    "    nodes = {}\n",
    "    # Edges is a hashmap from edges to a tuple with their capacity and cost\n",
    "    edges = {}\n",
    "\n",
    "    for line in file:\n",
    "        if len(line) > 0 and line[0] not in ignore_list:\n",
    "            if line[0] == 'n':\n",
    "                # Node parsing\n",
    "                node = [int(elem) for elem in line.split(' ')[1:]]\n",
    "                nodes[node[0]] = node[1]\n",
    "            elif line[0] == 'a':\n",
    "                arc = [int(elem) for elem in line.split(' ')[1:]]\n",
    "                node1 = arc[0]\n",
    "                node2 = arc[1]\n",
    "                capacity = arc[3]\n",
    "                cost = arc[4]\n",
    "\n",
    "                # Only nodes with non-zero supply are in a \"node line\"\n",
    "                if node1 not in nodes:\n",
    "                    nodes[node1] = 0\n",
    "                if node2 not in nodes:\n",
    "                    nodes[node2] = 0\n",
    "                if (node1, node2) in edges:\n",
    "                    # TODO not amazing (reaverages every time)\n",
    "                    old_capacity, old_cost = edges[(node1, node2)]\n",
    "                    new_cost = old_cost * old_capacity + cost * capacity\n",
    "                    new_cost /= (old_capacity + capacity)\n",
    "                    edges[(node1, node2)] = (old_capacity + capacity, new_cost)\n",
    "                else:\n",
    "                    edges[(node1, node2)] = (capacity, cost)\n",
    "    file.close()\n",
    "\n",
    "    capacities, costs = zip(*edges.values())\n",
    "    network = Network(list(nodes.keys()), list(edges.keys()), capacities, costs, list(nodes.values()))\n",
    "    #TODO data types?\n",
    "    print(f\"This dataset contains: {len(nodes.keys())} nodes and {len(edges.keys())} edges\")\n",
    "    if len(edges.keys()) <= 1e6:\n",
    "        index = {node: index for node, index in zip(nodes, range(len(nodes)))}\n",
    "        x = torch.tensor([supply for supply in nodes.values()])\n",
    "        edge_index = torch.reshape(torch.tensor([[index[e[0]], index[e[1]]] for e in edges]), (2, -1))\n",
    "        edge_attr = torch.reshape(torch.tensor([list(attributes) for attributes in edges.values()]), (2, -1))\n",
    "        print(\"starting to run successive shortest paths\")\n",
    "        iter_limit = 150\n",
    "        converged, f, p = successive_shortest_paths(network, iter_limit = iter_limit)\n",
    "        print(\"finished running successive shortest paths\")\n",
    "        y = dual_value(network, p)\n",
    "        if converged:\n",
    "            return {\"converged\": True, \"x\": x, \"edge_index\": edge_index, \"edge_attr\": edge_attr, \"y\": y}\n",
    "    return {\"converged\": False}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, download_url\n",
    "\n",
    "\n",
    "class MinCostDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"If these files are found in the raw directory, download is skipped\"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"If these files are found in the processed directory, processing is skipped\"\"\"\n",
    "        processed_files = []\n",
    "        path = self.processed_dir\n",
    "        for file in tqdm(os.listdir(path)):\n",
    "            file_path = os.path.join(path, file)\n",
    "            if not os.path.isdir(file_path) and not file == \"pre_filter.pt\" and not file == \"pre_transform.pt\":\n",
    "                processed_files.append(file)\n",
    "\n",
    "        return processed_files\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        path = self.raw_dir\n",
    "        for file in tqdm(os.listdir(path)):\n",
    "            print(file)\n",
    "            file_path = os.path.join(path, file)\n",
    "            if os.path.isdir(file_path):\n",
    "                continue\n",
    "            # Read data from `raw_path`.\n",
    "            output = parse(file_path)\n",
    "            if output[\"converged\"]:\n",
    "                x = output[\"x\"]\n",
    "                edge_index = output[\"edge_index\"]\n",
    "                edge_attr = output[\"edge_attr\"]\n",
    "                y = output[\"y\"]\n",
    "                data = Data(x = x, edge_index = edge_index, edge_attr = edge_attr, y = y, filename = file_path)\n",
    "\n",
    "                torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "                idx += 1\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 95837.59it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MinCostDataset(root = \"./data/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 14467.52it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 20054.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinCostDataset(32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 21383.47it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 30399.99it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 23393.43it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 26780.53it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 17123.72it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 25644.01it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 43517.34it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 35911.95it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 36185.32it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 33562.33it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 35527.24it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 44164.24it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 46300.76it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 41795.53it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 36915.96it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 140084.81it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 46956.32it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 37468.82it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 56366.14it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 56055.95it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 42455.00it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 47583.03it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 42278.78it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 47267.60it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 53171.64it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 70284.05it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 79981.12it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 61468.25it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 57829.01it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 82288.71it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 78918.84it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 60761.11it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 62822.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 60400.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 54284.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first graph: Data(x=[9559, 1], edge_index=[2, 29682], edge_attr=[29682, 2], y=[1, 1], filename='data/raw/road_flow_01_DC_a.txt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def dataset_information(dataset):\n",
    "    print(dataset)\n",
    "    print(f\"num classes: {dataset.num_classes}\")\n",
    "    print(f\"num features: {dataset.num_features}\")\n",
    "    print(f\"first graph: {dataset[0]}\")\n",
    "\n",
    "dataset_information(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 31008.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[9559, 1], edge_index=[2, 29682], edge_attr=[29682, 2], y=[1, 1], filename='data/raw/road_flow_01_DC_a.txt')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 56815.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].is_undirected())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_geometric.nn import NNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBN(torch.nn.Module):\n",
    "    #TODO cite the colab\n",
    "    def __init__(self, input_dim, output_dim, edge_feature_dim, args):\n",
    "        super(CBN, self).__init__()\n",
    "\n",
    "        hidden_dim = args.hidden_dim\n",
    "        num_layers = args.num_layers\n",
    "        dropout = args.dropout\n",
    "\n",
    "        if num_layers > 1:\n",
    "            conv_modules = [NNConv(input_dim, hidden_dim, nn.Linear(edge_feature_dim, input_dim * hidden_dim))]\n",
    "            conv_modules.extend([NNConv(hidden_dim, hidden_dim, nn.Linear(edge_feature_dim, hidden_dim * hidden_dim)) for _ in range(num_layers - 2)])\n",
    "            conv_modules.append(NNConv(hidden_dim, output_dim, nn.Linear(edge_feature_dim, hidden_dim * output_dim)))\n",
    "\n",
    "            self.convs = nn.ModuleList(conv_modules)\n",
    "        else:\n",
    "            self.convs = nn.ModuleList([NNConv(input_dim, output_dim, nn.Linear(edge_feature_dim, input_dim * output_dim))])\n",
    "\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # self.post_mp = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        # self.post_mp.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "\n",
    "        # x = self.post_mp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def dual_value(N, p):\n",
    "        return -np.sum([p[i] * N.b[i] for i in N.V]) - np.sum([N.u[e] * max(0, p[e[1]] - p[e[0]] - N.c[e]) for e in N.E])\n",
    "\n",
    "    def loss(self, pred, label, x, edge_index, edge_attr):\n",
    "        # edge_attr[0] is capacity, edge_attr[1] is cost\n",
    "        return label - torch.dot(pred, x) - torch.dot(edge_attr[:, 0], F.relu(pred[edge_index[:, 0]] - pred[edge_index[:, 0]] - edge_attr[:, 1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args):\n",
    "\n",
    "    print(\"Node task. test set size:\", np.sum(dataset[0]['test_mask'].numpy()))\n",
    "    print()\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    output_dim = 1 # we predict scalar potential values for each vertex\n",
    "    model = CBN(dataset.num_node_features, output_dim, dataset.num_edge_features, args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            print(f\"BATCH {batch}\")\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            print(f\"BATCH y: {batch.y.shape}\")\n",
    "            # pred = pred[batch.train_mask]\n",
    "            # label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "\n",
    "    return test_accs, losses, best_model, best_acc, test_loader\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
    "    test_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    # Note that Cora is only one graph!\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            pred = test_model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = label[mask]\n",
    "\n",
    "        if save_model_preds:\n",
    "          print (\"Saving Model Predictions for Model Type\", model_type)\n",
    "\n",
    "          data = {}\n",
    "          data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "          data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "          df = pd.DataFrame(data=data)\n",
    "          # Save locally as csv\n",
    "          df.to_csv('MinCostFlow-' + model_type + '.csv', sep=',', index=False)\n",
    "\n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, loss_fn):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index, data.edge_attr)\n",
    "    label = data.y\n",
    "    loss = loss_fn(out, label)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test function here\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
    "    # TODO: Implement a function that tests the model by\n",
    "    # using the given split_idx and evaluator.\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.adj_t, data.edge_attr)\n",
    "\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y,\n",
    "        'y_pred': y_pred,\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y,\n",
    "        'y_pred': y_pred,\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y,\n",
    "        'y_pred': y_pred,\n",
    "    })['acc']\n",
    "\n",
    "    if save_model_results:\n",
    "      print (\"Saving Model Predictions\")\n",
    "\n",
    "      data = {'y_pred': y_pred.view(-1).cpu().detach().numpy()}\n",
    "\n",
    "      df = pd.DataFrame(data=data)\n",
    "      # Save locally as csv\n",
    "      df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "    return train_acc, valid_acc, test_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
