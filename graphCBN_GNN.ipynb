{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of edges: 24\n",
      "Number of flow updates: 5, final flow value: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandre/miniconda3/envs/cs224w/lib/python3.10/site-packages/nbformat/__init__.py:92: MissingIDFieldWarning: Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "  validate(nb)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/tmp/ipykernel_3247/1483371613.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m nodes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m]\n\u001B[1;32m      6\u001B[0m N \u001B[38;5;241m=\u001B[39m Network(nodes, edges, capacities, costs, supplies)     \n\u001B[0;32m----> 8\u001B[0m f, p \u001B[38;5;241m=\u001B[39m successive_shortest_paths(N)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrun\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msuccessive_shortest_paths.ipynb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2369\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2367\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[1;32m   2368\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[0;32m-> 2369\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2370\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/IPython/core/magics/execution.py:717\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[1;32m    715\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m preserve_keys(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39muser_ns, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__file__\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m    716\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshell\u001B[38;5;241m.\u001B[39muser_ns[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__file__\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m filename\n\u001B[0;32m--> 717\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshell\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msafe_execfile_ipy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraise_exceptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    718\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m    720\u001B[0m \u001B[38;5;66;03m# Control the response to exit() calls made by the script being run\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2875\u001B[0m, in \u001B[0;36mInteractiveShell.safe_execfile_ipy\u001B[0;34m(self, fname, shell_futures, raise_exceptions)\u001B[0m\n\u001B[1;32m   2873\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_cell(cell, silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, shell_futures\u001B[38;5;241m=\u001B[39mshell_futures)\n\u001B[1;32m   2874\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m raise_exceptions:\n\u001B[0;32m-> 2875\u001B[0m     \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2876\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result\u001B[38;5;241m.\u001B[39msuccess:\n\u001B[1;32m   2877\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/IPython/core/interactiveshell.py:266\u001B[0m, in \u001B[0;36mExecutionResult.raise_error\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_before_exec\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_in_exec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 266\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_in_exec\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m/tmp/ipykernel_3247/1483371613.py:8\u001B[0m\n\u001B[1;32m      5\u001B[0m nodes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m]\n\u001B[1;32m      6\u001B[0m N \u001B[38;5;241m=\u001B[39m Network(nodes, edges, capacities, costs, supplies)     \n\u001B[0;32m----> 8\u001B[0m f, p \u001B[38;5;241m=\u001B[39m successive_shortest_paths(N)\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "%run successive_shortest_paths.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def parse(filename) -> Network:\n",
    "    \"\"\"\n",
    "    Parses a network file following the DIMACS problem specification\n",
    "    structure and transforms it into a Network object\n",
    "\n",
    "    Some elements of the specification:\n",
    "    - Lines starting in c are comments\n",
    "    - Lines starting in p explain what problem to solve (can be ignored,\n",
    "      we only consider minimum-cost flow problems)\n",
    "    - Lines starting in n define nodes\n",
    "    - Lines starting in a define arcs (edges)\n",
    "\n",
    "    Args:\n",
    "        filename: name of the file containing the network data\n",
    "\n",
    "    Returns:\n",
    "        The corresponding Network object\n",
    "    \"\"\"\n",
    "    # Lines we can ignore\n",
    "    ignore_list = ['c', 'p']\n",
    "\n",
    "    file = open(filename, 'r')\n",
    "\n",
    "    # Nodes is a hashmap from node values to their supply\n",
    "    nodes = {}\n",
    "    # Edges is a hashmap from edges to a tuple with their capacity and cost\n",
    "    edges = {}\n",
    "\n",
    "    for line in file:\n",
    "        if len(line) > 0 and line[0] not in ignore_list:\n",
    "            if line[0] == 'n':\n",
    "                # Node parsing\n",
    "                node = [int(elem) for elem in line.split(' ')[1:]]\n",
    "                nodes[node[0]] = node[1]\n",
    "            elif line[0] == 'a':\n",
    "                arc = [int(elem) for elem in line.split(' ')[1:]]\n",
    "                node1 = arc[0]\n",
    "                node2 = arc[1]\n",
    "                capacity = arc[3]\n",
    "                cost = arc[4]\n",
    "\n",
    "                # Only nodes with non-zero supply are in a \"node line\"\n",
    "                if node1 not in nodes:\n",
    "                    nodes[node1] = 0\n",
    "                if node2 not in nodes:\n",
    "                    nodes[node2] = 0\n",
    "                if (node1, node2) in edges:\n",
    "                    # TODO not amazing (reaverages every time)\n",
    "                    old_capacity, old_cost = edges[(node1, node2)]\n",
    "                    new_cost = old_cost * old_capacity + cost * capacity\n",
    "                    new_cost /= (old_capacity + capacity)\n",
    "                    edges[(node1, node2)] = (old_capacity + capacity, new_cost)\n",
    "                else:\n",
    "                    edges[(node1, node2)] = (capacity, cost)\n",
    "    file.close()\n",
    "\n",
    "    capacities, costs = zip(*edges.values())\n",
    "    network = Network(list(nodes.keys()), list(edges.keys()), capacities, costs, list(nodes.values()))\n",
    "    #TODO data types?\n",
    "    print(f\"This dataset contains: {len(nodes.keys())} nodes and {len(edges.keys())} edges\")\n",
    "    if len(edges.keys()) <= 1e6:\n",
    "        index = {node: index for node, index in zip(nodes, range(len(nodes)))}\n",
    "        x = torch.tensor([supply for supply in nodes.values()])\n",
    "        edge_index = torch.reshape(torch.tensor([[index[e[0]], index[e[1]]] for e in edges]), (2, -1))\n",
    "        edge_attr = torch.reshape(torch.tensor([list(attributes) for attributes in edges.values()]), (2, -1))\n",
    "        print(\"starting to run successive shortest paths\")\n",
    "        iter_limit = 150\n",
    "        converged, f, p = successive_shortest_paths(network, iter_limit = iter_limit)\n",
    "        print(\"finished running successive shortest paths\")\n",
    "        y = dual_value(network, p)\n",
    "        if converged:\n",
    "            return {\"converged\": True, \"x\": x, \"edge_index\": edge_index, \"edge_attr\": edge_attr, \"y\": y}\n",
    "    return {\"converged\": False}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset, download_url\n",
    "\n",
    "\n",
    "class MinCostDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"If these files are found in the raw directory, download is skipped\"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"If these files are found in the processed directory, processing is skipped\"\"\"\n",
    "        processed_files = []\n",
    "        path = self.processed_dir\n",
    "        for file in tqdm(os.listdir(path)):\n",
    "            file_path = os.path.join(path, file)\n",
    "            if not os.path.isdir(file_path) and not file == \"pre_filter.pt\" and not file == \"pre_transform.pt\":\n",
    "                processed_files.append(file)\n",
    "\n",
    "        return processed_files\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        path = self.raw_dir\n",
    "        for file in tqdm(os.listdir(path)):\n",
    "            print(file)\n",
    "            file_path = os.path.join(path, file)\n",
    "            if os.path.isdir(file_path):\n",
    "                continue\n",
    "            # Read data from `raw_path`.\n",
    "            output = parse(file_path)\n",
    "            if output[\"converged\"]:\n",
    "                x = output[\"x\"]\n",
    "                edge_index = output[\"edge_index\"]\n",
    "                edge_attr = output[\"edge_attr\"]\n",
    "                y = output[\"y\"]\n",
    "                data = Data(x = x, edge_index = edge_index, edge_attr = edge_attr, y = y, filename = file_path)\n",
    "\n",
    "                torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "                idx += 1\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 48804.36it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = MinCostDataset(root = \"./data/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 94629.29it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 90543.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinCostDataset(32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 42696.51it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 72647.14it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 55814.61it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 96682.26it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 95134.31it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 77884.40it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 77251.54it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 58373.45it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 77167.93it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 45329.41it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 46225.72it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 62933.07it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 49967.18it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 50731.53it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 85138.11it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 61868.26it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 52719.53it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 50840.05it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 82718.29it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 156195.33it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 88740.72it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 80751.04it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 77209.71it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 55814.61it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 94944.30it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 78054.92it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 75055.97it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 97209.50it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 80206.04it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 59692.90it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 63834.53it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 94254.02it/s]\n",
      "100%|██████████| 34/34 [00:00<00:00, 55836.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 86428.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 56522.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first graph: Data(x=[9559, 1], edge_index=[2, 29682], edge_attr=[29682, 2], y=[1, 1], filename='data/raw/road_flow_01_DC_a.txt')\n"
     ]
    }
   ],
   "source": [
    "def dataset_information(dataset):\n",
    "    print(dataset)\n",
    "    print(f\"num classes: {dataset.num_classes}\")\n",
    "    print(f\"num features: {dataset.num_features}\")\n",
    "    print(f\"first graph: {dataset[0]}\")\n",
    "\n",
    "dataset_information(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "from torch_geometric.nn import NNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBN(torch.nn.Module):\n",
    "    #TODO cite the colab\n",
    "    def __init__(self, input_dim, output_dim, edge_feature_dim, args):\n",
    "        super(CBN, self).__init__()\n",
    "\n",
    "        hidden_dim = args.hidden_dim\n",
    "        num_layers = args.num_layers\n",
    "        dropout = args.dropout\n",
    "\n",
    "        if num_layers > 1:\n",
    "            conv_modules = [NNConv(input_dim, hidden_dim, nn.Linear(edge_feature_dim, input_dim * hidden_dim))]\n",
    "            conv_modules.extend([NNConv(hidden_dim, hidden_dim, nn.Linear(edge_feature_dim, hidden_dim * hidden_dim)) for _ in range(num_layers - 2)])\n",
    "            conv_modules.append(NNConv(hidden_dim, output_dim, nn.Linear(edge_feature_dim, hidden_dim * output_dim)))\n",
    "\n",
    "            self.convs = nn.ModuleList(conv_modules)\n",
    "        else:\n",
    "            self.convs = nn.ModuleList([NNConv(input_dim, output_dim, nn.Linear(edge_feature_dim, input_dim * output_dim))])\n",
    "\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # self.post_mp = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        # self.post_mp.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "\n",
    "        # x = self.post_mp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def dual_value(N, p):\n",
    "        return -np.sum([p[i] * N.b[i] for i in N.V]) - np.sum([N.u[e] * max(0, p[e[1]] - p[e[0]] - N.c[e]) for e in N.E])\n",
    "\n",
    "    def loss(self, pred, label, x, edge_index, edge_attr):\n",
    "        # edge_attr[0] is capacity, edge_attr[1] is cost\n",
    "        print(pred.shape)\n",
    "        print(edge_index[0].shape)\n",
    "        print(pred[edge_index[1]].shape)\n",
    "        print(edge_attr[:, 1].shape)\n",
    "        reduced_cost = pred[edge_index[1]].squeeze() - pred[edge_index[0]].squeeze() - edge_attr[:, 1]\n",
    "        print(reduced_cost.shape)\n",
    "        return label - torch.dot(pred.squeeze(), x.squeeze()) - torch.dot(edge_attr[:, 0], F.relu(reduced_cost))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "class DualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DualLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, label, x, edge_index, edge_attr):\n",
    "        # edge_attr[0] is capacity, edge_attr[1] is cost\n",
    "        #TODO is negative for the moment but if you switch the sign before the second dot product it's always positive ._.\n",
    "        reduced_cost = pred[edge_index[1]].squeeze() - pred[edge_index[0]].squeeze() - edge_attr[:, 1]\n",
    "        return label - torch.dot(pred.squeeze(), x.squeeze()) - torch.dot(edge_attr[:, 0], F.relu(reduced_cost))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 43305.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 638121991.6210938\n",
      "loss: 471209803.6875\n",
      "loss: 440709712.5625\n",
      "loss: 524851962.78125\n",
      "loss: 165086177.1875\n",
      "loss: 371463807.5625\n",
      "loss: 120730797.625\n",
      "loss: 209988681.0\n",
      "loss: 104120671.125\n",
      "loss: 133735759.6875\n",
      "loss: 103533169.25\n",
      "loss: 107476511.375\n",
      "loss: 102671376.75\n",
      "loss: 102371204.25\n",
      "loss: 174341024.625\n",
      "loss: 130944314.0\n",
      "loss: 120736630.75\n",
      "loss: 107378604.5\n",
      "loss: 103262994.75\n",
      "loss: 98563413.3125\n",
      "loss: 101386237.625\n",
      "loss: 151278697.625\n",
      "loss: 222132751.375\n",
      "loss: 131472010.25\n",
      "loss: 138911438.375\n",
      "loss: 115348867.25\n",
      "loss: 168915868.3125\n",
      "loss: 113928190.625\n",
      "loss: 107887706.875\n",
      "loss: 97301938.5\n",
      "loss: 120312248.5\n",
      "loss: 113747868.75\n",
      "loss: 112466291.375\n",
      "loss: 109370741.1875\n",
      "loss: 119142837.625\n",
      "loss: 105108579.25\n",
      "loss: 138285597.8125\n",
      "loss: 115640512.0\n",
      "loss: 109481616.875\n",
      "loss: 106230935.375\n",
      "loss: 156791418.5\n",
      "loss: 117160115.9375\n",
      "loss: 110349376.0625\n",
      "loss: 116201603.0625\n",
      "loss: 110626329.8125\n",
      "loss: 107294813.375\n",
      "loss: 120373888.25\n",
      "loss: 107356042.75\n",
      "loss: 103801544.1875\n",
      "loss: 121649612.5\n",
      "loss: 113355003.0\n",
      "loss: 115824879.75\n",
      "loss: 125309611.375\n",
      "loss: 101558523.5625\n",
      "loss: 97759743.25\n",
      "loss: 118455871.875\n",
      "loss: 111445624.75\n",
      "loss: 109178375.4375\n",
      "loss: 109567000.75\n",
      "loss: 104542928.96875\n",
      "loss: 107812557.78125\n",
      "loss: 124450025.75\n",
      "loss: 112813422.0\n",
      "loss: 120392631.71875\n",
      "loss: 106036846.9375\n",
      "loss: 101357682.375\n",
      "loss: 101104988.75\n",
      "loss: 98559913.4375\n",
      "loss: 109959188.125\n",
      "loss: 115705758.0625\n",
      "loss: 101892519.5\n",
      "loss: 107100034.375\n",
      "loss: 96597273.5\n",
      "loss: 113196565.6875\n",
      "loss: 102709480.625\n",
      "loss: 99260461.8125\n",
      "loss: 100427482.0625\n",
      "loss: 106089424.875\n",
      "loss: 103427653.0\n",
      "loss: 104864175.75\n",
      "loss: 103570294.28125\n",
      "loss: 104328124.3125\n",
      "loss: 105332179.625\n",
      "loss: 104624548.53125\n",
      "loss: 98085494.8125\n",
      "loss: 115629859.125\n",
      "loss: 101092679.9375\n",
      "loss: 165608386.65625\n",
      "loss: 122867948.84375\n",
      "loss: 116465528.28125\n",
      "loss: 108552074.6875\n",
      "loss: 98060848.625\n",
      "loss: 113921385.25\n",
      "loss: 105789972.65625\n",
      "loss: 97985801.65625\n",
      "loss: 124889049.25\n",
      "loss: 108598090.1875\n",
      "loss: 110128455.25\n",
      "loss: 107568515.75\n",
      "loss: 102830377.59375\n",
      "tensor([0.0000e+00, 0.0000e+00, 7.3662e-03, 0.0000e+00, 1.5159e-03, 0.0000e+00,\n",
      "        0.0000e+00, 2.4794e-03, 4.8869e-03, 7.3662e-03, 2.4794e-03, 1.5159e-03,\n",
      "        3.9953e-03, 0.0000e+00, 0.0000e+00, 2.4794e-03, 1.5159e-03, 1.5159e-03,\n",
      "        3.9953e-03, 4.8869e-03, 2.4794e-03, 1.5159e-03, 0.0000e+00, 0.0000e+00,\n",
      "        1.5159e-03, 4.8869e-03, 2.4794e-03, 0.0000e+00, 1.5159e-03, 3.9953e-03,\n",
      "        3.9953e-03, 2.4794e-03, 0.0000e+00, 9.0679e+01, 0.0000e+00, 3.9953e-03,\n",
      "        7.3662e-03, 7.3662e-03, 2.4794e-03, 2.4794e-03, 2.4794e-03, 7.3662e-03,\n",
      "        7.3662e-03, 2.4794e-03, 3.9953e-03, 2.4794e-03, 2.4794e-03, 0.0000e+00,\n",
      "        3.9953e-03, 3.9953e-03, 1.5159e-03, 0.0000e+00, 4.8869e-03, 2.4794e-03,\n",
      "        2.4794e-03, 0.0000e+00, 2.4794e-03, 1.5159e-03, 7.3662e-03, 2.4794e-03,\n",
      "        1.5159e-03, 0.0000e+00, 7.3662e-03, 0.0000e+00, 2.4794e-03, 2.4794e-03,\n",
      "        1.5159e-03, 1.5159e-03, 2.4794e-03, 1.5159e-03, 1.5159e-03, 0.0000e+00,\n",
      "        4.8869e-03, 3.9953e-03, 4.8869e-03, 4.8869e-03, 7.3662e-03, 4.8869e-03,\n",
      "        2.4794e-03, 1.5159e-03, 1.5159e-03, 3.9953e-03, 0.0000e+00, 2.4794e-03,\n",
      "        7.3662e-03, 7.3662e-03, 7.3662e-03, 7.3662e-03, 1.5159e-03, 7.3662e-03,\n",
      "        2.4794e-03, 3.9953e-03, 3.9953e-03, 2.4794e-03, 3.9953e-03, 7.3662e-03,\n",
      "        0.0000e+00, 7.3662e-03, 3.9953e-03, 1.5159e-03, 3.9953e-03, 1.5159e-03,\n",
      "        1.5159e-03, 0.0000e+00, 0.0000e+00, 7.3662e-03, 1.5159e-03, 1.5159e-03,\n",
      "        0.0000e+00, 3.9953e-03, 1.5159e-03, 1.5159e-03, 0.0000e+00, 7.3662e-03,\n",
      "        3.9953e-03, 4.8869e-03, 3.9953e-03, 1.5159e-03, 3.9953e-03, 1.5159e-03,\n",
      "        2.4794e-03, 1.5159e-03, 0.0000e+00, 1.5159e-03, 4.8869e-03, 7.3662e-03,\n",
      "        4.8869e-03, 4.8869e-03, 1.5159e-03, 4.8869e-03, 3.9953e-03, 1.5159e-03,\n",
      "        0.0000e+00, 3.9953e-03, 4.8869e-03, 0.0000e+00, 7.3662e-03, 3.9953e-03,\n",
      "        0.0000e+00, 0.0000e+00, 1.5159e-03, 1.5159e-03, 7.3662e-03, 4.8869e-03,\n",
      "        1.5159e-03, 7.3662e-03, 1.5159e-03, 0.0000e+00, 0.0000e+00, 4.8869e-03,\n",
      "        0.0000e+00, 4.8869e-03, 2.4794e-03, 2.4794e-03, 3.9953e-03, 4.8869e-03,\n",
      "        7.3662e-03, 7.3662e-03, 1.5159e-03, 3.9953e-03, 2.4794e-03, 7.3662e-03,\n",
      "        0.0000e+00, 0.0000e+00, 4.8869e-03, 0.0000e+00, 1.5159e-03, 3.9953e-03,\n",
      "        7.3662e-03, 4.8869e-03, 3.9953e-03, 4.8869e-03, 4.8869e-03, 3.9953e-03,\n",
      "        3.9953e-03, 2.4794e-03, 3.9953e-03, 7.3662e-03, 7.3662e-03, 4.8869e-03,\n",
      "        2.4794e-03, 1.5159e-03, 4.8869e-03, 4.8869e-03, 4.8869e-03, 1.5159e-03,\n",
      "        2.4794e-03, 4.8869e-03, 2.4794e-03, 0.0000e+00, 0.0000e+00, 1.5159e-03,\n",
      "        3.9953e-03, 4.8869e-03, 1.5159e-03, 7.3662e-03, 0.0000e+00, 1.5159e-03,\n",
      "        7.3662e-03, 2.4794e-03, 0.0000e+00, 7.3662e-03, 0.0000e+00, 0.0000e+00,\n",
      "        1.5159e-03, 0.0000e+00, 1.5159e-03, 7.3662e-03, 3.9953e-03, 3.9953e-03,\n",
      "        4.8869e-03, 0.0000e+00, 3.9953e-03, 3.9953e-03, 3.9953e-03, 1.5159e-03,\n",
      "        1.5159e-03, 1.5159e-03, 7.3662e-03, 4.8869e-03, 0.0000e+00, 3.9953e-03,\n",
      "        4.8869e-03, 0.0000e+00, 4.8869e-03, 7.3662e-03, 4.8869e-03, 4.8869e-03,\n",
      "        2.4794e-03, 3.9953e-03, 0.0000e+00, 0.0000e+00, 7.3662e-03, 1.5159e-03,\n",
      "        1.5159e-03, 0.0000e+00, 0.0000e+00, 1.5159e-03, 3.9953e-03, 4.8869e-03,\n",
      "        7.3662e-03, 1.5159e-03, 2.4794e-03, 4.8869e-03, 0.0000e+00, 0.0000e+00,\n",
      "        7.3662e-03, 0.0000e+00, 2.4794e-03, 4.8869e-03, 1.5159e-03, 1.5159e-03,\n",
      "        0.0000e+00, 2.4794e-03, 2.4794e-03, 3.9953e-03, 7.3662e-03, 7.3662e-03,\n",
      "        1.5159e-03, 0.0000e+00, 3.9953e-03, 1.5159e-03, 2.4794e-03, 4.8869e-03,\n",
      "        1.5159e-03, 0.0000e+00, 2.4794e-03, 3.9953e-03, 3.9953e-03, 3.9953e-03,\n",
      "        7.3662e-03, 3.9953e-03, 1.5159e-03, 2.4794e-03, 4.8869e-03, 2.4794e-03,\n",
      "        1.5159e-03, 7.3662e-03, 3.9953e-03, 0.0000e+00, 7.3662e-03, 1.5159e-03,\n",
      "        3.9953e-03, 1.5159e-03, 7.3662e-03, 4.8869e-03, 2.4794e-03, 4.8869e-03,\n",
      "        3.9953e-03, 0.0000e+00, 4.8869e-03, 1.5159e-03, 4.8869e-03, 4.8869e-03,\n",
      "        3.9953e-03, 2.4794e-03, 2.4794e-03, 4.8869e-03, 7.3662e-03, 3.9953e-03,\n",
      "        2.4794e-03, 3.9953e-03, 4.8869e-03, 1.5159e-03, 0.0000e+00, 4.8869e-03,\n",
      "        4.8869e-03, 3.9953e-03, 3.9953e-03, 3.9953e-03, 0.0000e+00, 4.8869e-03,\n",
      "        2.4794e-03, 2.4794e-03, 3.9953e-03, 3.9953e-03, 7.3662e-03, 1.5159e-03,\n",
      "        0.0000e+00, 2.4794e-03, 7.3662e-03, 3.9953e-03, 2.4794e-03, 4.8869e-03,\n",
      "        2.4794e-03, 0.0000e+00, 2.4794e-03, 4.8869e-03, 3.9953e-03, 0.0000e+00,\n",
      "        2.4794e-03, 2.4794e-03, 3.9953e-03, 1.5159e-03, 7.3662e-03, 3.9953e-03,\n",
      "        1.5159e-03, 7.3662e-03, 2.4794e-03, 7.3662e-03, 0.0000e+00, 1.5159e-03,\n",
      "        7.3662e-03, 1.5159e-03, 3.9953e-03, 7.3662e-03, 3.9953e-03, 1.5159e-03,\n",
      "        1.5159e-03, 7.3662e-03, 7.3662e-03, 7.3662e-03, 2.4794e-03, 3.9953e-03,\n",
      "        3.9953e-03, 3.9953e-03, 2.4794e-03, 2.4794e-03, 4.8869e-03, 0.0000e+00,\n",
      "        2.4794e-03, 3.9953e-03, 4.8869e-03, 4.8869e-03, 2.4794e-03, 3.9953e-03,\n",
      "        3.9953e-03, 4.8869e-03, 4.8869e-03, 7.3662e-03, 4.8869e-03, 0.0000e+00,\n",
      "        3.9953e-03, 1.5159e-03, 3.9953e-03, 3.9953e-03, 0.0000e+00, 4.8869e-03,\n",
      "        3.9953e-03, 2.4794e-03, 0.0000e+00, 7.3662e-03, 1.5159e-03, 3.9953e-03,\n",
      "        0.0000e+00, 1.5159e-03, 2.4794e-03, 3.9953e-03, 7.3662e-03, 4.8869e-03,\n",
      "        3.9953e-03, 1.5159e-03, 0.0000e+00, 7.3662e-03, 1.5159e-03, 0.0000e+00,\n",
      "        3.9953e-03, 0.0000e+00, 2.4794e-03, 4.8869e-03, 2.4794e-03, 2.4794e-03,\n",
      "        4.8869e-03, 0.0000e+00, 2.4794e-03, 4.8869e-03, 4.8869e-03, 0.0000e+00,\n",
      "        3.9953e-03, 0.0000e+00, 1.5159e-03, 1.5159e-03, 3.9953e-03, 1.5159e-03,\n",
      "        4.8869e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.3662e-03,\n",
      "        0.0000e+00, 4.8869e-03, 4.8869e-03, 3.9953e-03, 3.9953e-03, 0.0000e+00,\n",
      "        7.3662e-03, 3.9953e-03, 1.5159e-03, 3.9953e-03, 3.9953e-03, 7.3662e-03,\n",
      "        1.5159e-03, 4.8869e-03, 2.4794e-03, 1.5159e-03, 7.3662e-03, 4.8869e-03,\n",
      "        2.4794e-03, 2.4794e-03, 4.8869e-03, 0.0000e+00, 4.8869e-03, 4.8869e-03,\n",
      "        3.9953e-03, 0.0000e+00, 1.5159e-03, 1.5159e-03, 7.3662e-03, 4.8869e-03,\n",
      "        4.8869e-03, 7.3662e-03, 4.8869e-03, 3.9953e-03, 2.4794e-03, 2.4794e-03,\n",
      "        4.8869e-03, 3.9953e-03, 2.4794e-03, 1.5159e-03, 1.5159e-03, 1.5159e-03,\n",
      "        1.5159e-03, 3.9953e-03, 0.0000e+00, 3.9953e-03, 1.5159e-03, 4.8869e-03,\n",
      "        2.4794e-03, 3.9953e-03, 7.3662e-03, 2.4794e-03, 2.4794e-03, 3.9953e-03,\n",
      "        1.5159e-03, 1.5159e-03, 0.0000e+00, 7.3662e-03, 2.4794e-03, 1.5159e-03,\n",
      "        7.3662e-03, 4.8869e-03, 3.9953e-03, 1.5159e-03, 1.5159e-03, 4.8869e-03,\n",
      "        3.9953e-03, 4.2161e+02, 0.0000e+00, 3.9953e-03, 2.4794e-03, 4.8869e-03,\n",
      "        4.8869e-03, 7.3662e-03, 2.4794e-03, 1.5159e-03, 4.8869e-03, 3.9953e-03,\n",
      "        1.5159e-03, 1.5159e-03, 4.8869e-03, 0.0000e+00, 0.0000e+00, 4.8869e-03,\n",
      "        3.9953e-03, 1.5159e-03, 4.8869e-03, 7.3662e-03, 3.9953e-03, 2.4794e-03,\n",
      "        3.9953e-03, 2.4794e-03, 1.5159e-03, 1.5159e-03, 3.9953e-03, 3.9953e-03,\n",
      "        4.8869e-03, 3.9953e-03, 1.5159e-03, 4.8869e-03, 2.4794e-03, 7.3662e-03,\n",
      "        0.0000e+00, 4.8869e-03, 3.9953e-03, 7.3662e-03, 7.3662e-03, 7.3662e-03,\n",
      "        0.0000e+00, 0.0000e+00, 2.4794e-03, 0.0000e+00, 3.9953e-03, 1.5159e-03,\n",
      "        1.5159e-03, 0.0000e+00, 0.0000e+00, 1.5159e-03, 7.3662e-03, 2.4794e-03,\n",
      "        7.3662e-03, 0.0000e+00, 4.8869e-03, 0.0000e+00, 2.4794e-03, 3.9953e-03,\n",
      "        3.9953e-03, 4.8869e-03, 0.0000e+00, 1.5159e-03, 1.5159e-03, 4.8869e-03,\n",
      "        7.3662e-03, 3.9953e-03, 1.5159e-03, 2.4794e-03, 2.4794e-03, 0.0000e+00,\n",
      "        2.4794e-03, 7.3662e-03, 2.4794e-03, 3.9953e-03, 0.0000e+00, 0.0000e+00,\n",
      "        1.5159e-03, 3.9953e-03, 4.8869e-03, 7.3662e-03, 4.8869e-03, 4.8869e-03,\n",
      "        1.5159e-03, 3.9953e-03, 0.0000e+00, 4.8869e-03, 1.5159e-03, 0.0000e+00,\n",
      "        2.4794e-03, 7.3662e-03, 0.0000e+00, 1.5159e-03, 0.0000e+00, 3.9953e-03,\n",
      "        3.9953e-03, 1.5159e-03, 7.3662e-03, 4.8869e-03, 2.4794e-03, 7.3662e-03,\n",
      "        3.9953e-03, 7.3662e-03, 4.8869e-03, 1.5159e-03, 2.4794e-03, 2.4794e-03,\n",
      "        0.0000e+00, 2.4794e-03, 2.4794e-03, 0.0000e+00, 4.8869e-03, 7.3662e-03,\n",
      "        7.3662e-03, 1.5159e-03, 3.9953e-03, 1.5159e-03, 1.5159e-03, 0.0000e+00,\n",
      "        7.3662e-03, 4.8869e-03, 0.0000e+00, 0.0000e+00, 4.8869e-03, 3.9953e-03,\n",
      "        1.5159e-03, 4.8869e-03, 3.9953e-03, 1.5159e-03, 2.4794e-03, 4.8869e-03,\n",
      "        1.5159e-03, 7.3662e-03, 4.8869e-03, 0.0000e+00, 2.4794e-03, 3.9953e-03,\n",
      "        1.5159e-03, 1.5159e-03, 0.0000e+00, 7.3662e-03, 4.8869e-03, 0.0000e+00,\n",
      "        1.5159e-03, 0.0000e+00, 1.5159e-03, 2.4794e-03, 0.0000e+00, 3.9953e-03,\n",
      "        0.0000e+00, 4.8869e-03, 2.4794e-03, 1.5159e-03, 7.3662e-03, 7.3662e-03,\n",
      "        7.3662e-03, 0.0000e+00, 4.8869e-03, 2.4794e-03, 7.3662e-03, 1.5159e-03,\n",
      "        3.9953e-03, 2.4794e-03, 7.3662e-03, 4.8869e-03, 0.0000e+00, 0.0000e+00,\n",
      "        2.4794e-03, 1.5159e-03, 7.3662e-03, 4.8869e-03, 7.3662e-03, 7.3662e-03,\n",
      "        1.5159e-03, 4.8869e-03, 2.4794e-03, 2.4794e-03, 1.5159e-03, 7.3662e-03,\n",
      "        1.5159e-03, 3.9953e-03, 1.5159e-03, 1.5159e-03, 4.8869e-03, 0.0000e+00,\n",
      "        1.5159e-03, 3.9953e-03, 3.9953e-03, 3.9953e-03, 4.8869e-03, 7.3662e-03,\n",
      "        2.4794e-03, 4.8869e-03, 3.9953e-03, 2.4794e-03, 2.4794e-03, 0.0000e+00,\n",
      "        1.5159e-03, 0.0000e+00, 2.4794e-03, 0.0000e+00, 2.4794e-03, 7.3662e-03,\n",
      "        4.8869e-03, 7.3662e-03, 1.5159e-03, 4.8869e-03, 2.4794e-03, 2.4794e-03,\n",
      "        7.3662e-03, 0.0000e+00, 7.3662e-03, 0.0000e+00, 4.8869e-03, 3.9953e-03,\n",
      "        1.5159e-03, 4.8869e-03, 1.5159e-03, 3.9953e-03, 3.9953e-03, 0.0000e+00,\n",
      "        2.4794e-03, 7.3662e-03, 2.4794e-03, 3.9953e-03, 0.0000e+00, 0.0000e+00,\n",
      "        1.5159e-03, 0.0000e+00, 0.0000e+00, 3.9953e-03, 2.4794e-03, 1.5159e-03,\n",
      "        7.3662e-03, 3.9953e-03, 3.9953e-03, 3.9953e-03, 2.4794e-03, 2.4794e-03,\n",
      "        3.9953e-03, 4.8869e-03, 3.9953e-03, 1.5159e-03, 2.4794e-03, 7.3662e-03,\n",
      "        2.4794e-03, 2.4794e-03, 0.0000e+00, 1.5159e-03, 3.9953e-03, 2.4794e-03,\n",
      "        3.9953e-03, 3.9953e-03, 7.3662e-03, 7.3662e-03, 4.8869e-03, 0.0000e+00,\n",
      "        1.5159e-03, 3.9953e-03, 0.0000e+00, 4.8869e-03, 7.3662e-03, 7.3662e-03,\n",
      "        7.3662e-03, 4.8869e-03, 1.5159e-03, 1.5159e-03, 7.3662e-03, 3.9953e-03,\n",
      "        2.4794e-03, 1.5159e-03, 3.9953e-03, 0.0000e+00, 4.8869e-03, 3.9953e-03,\n",
      "        2.4794e-03, 2.4794e-03, 3.9953e-03, 1.5159e-03, 0.0000e+00, 3.9953e-03,\n",
      "        7.3662e-03, 7.3662e-03, 3.9953e-03, 7.3662e-03, 1.5159e-03, 4.8869e-03,\n",
      "        4.8869e-03, 2.4794e-03, 7.3662e-03, 3.9953e-03, 0.0000e+00, 2.4794e-03,\n",
      "        7.3662e-03, 3.9953e-03, 3.9953e-03, 7.3662e-03, 4.8869e-03, 0.0000e+00,\n",
      "        2.4794e-03, 2.4794e-03, 7.3662e-03, 2.4794e-03, 3.9953e-03, 4.8869e-03,\n",
      "        3.9953e-03, 4.8869e-03, 7.3662e-03, 1.5159e-03, 3.9953e-03, 0.0000e+00,\n",
      "        2.4794e-03, 3.9953e-03, 2.4794e-03, 1.5159e-03, 0.0000e+00, 7.3662e-03,\n",
      "        0.0000e+00, 3.9953e-03, 0.0000e+00, 3.9953e-03, 4.8869e-03, 3.9953e-03,\n",
      "        4.8869e-03, 7.3662e-03, 2.4794e-03, 7.3662e-03, 3.9953e-03, 3.9953e-03,\n",
      "        3.9953e-03, 4.8869e-03, 1.5159e-03, 1.5159e-03, 2.4794e-03, 7.3662e-03,\n",
      "        0.0000e+00, 4.8869e-03, 2.4794e-03, 0.0000e+00, 7.3662e-03, 0.0000e+00,\n",
      "        4.8869e-03, 0.0000e+00, 3.9953e-03, 1.5159e-03, 7.3662e-03, 4.8869e-03,\n",
      "        1.5159e-03, 3.9953e-03, 1.5159e-03, 2.4794e-03, 4.8869e-03, 2.4794e-03,\n",
      "        1.5159e-03, 1.5159e-03, 0.0000e+00, 7.3662e-03, 0.0000e+00, 7.3662e-03,\n",
      "        3.9953e-03, 3.9953e-03, 6.5101e+01, 1.5159e-03, 7.3662e-03, 0.0000e+00,\n",
      "        3.9953e-03, 7.3662e-03, 4.8869e-03, 3.9953e-03, 2.4794e-03, 2.4794e-03,\n",
      "        1.5159e-03, 0.0000e+00, 0.0000e+00, 7.3662e-03, 4.8869e-03, 4.8869e-03,\n",
      "        1.5159e-03, 3.9953e-03, 3.9953e-03, 7.3662e-03, 0.0000e+00, 2.4794e-03,\n",
      "        7.3662e-03, 1.2496e+02, 6.4785e+01, 1.4024e+01, 1.1529e+01, 5.4516e+01,\n",
      "        7.3662e-03, 1.9531e+01, 2.2466e+01, 2.0031e+01, 8.6909e+01],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "args = {'num_layers': 2, 'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01}\n",
    "args = objectview(args)\n",
    "model = CBN(1, 1, 2, args)\n",
    "loss_fn = DualLoss()\n",
    "data = dataset[0]\n",
    "\n",
    "\n",
    "scheduler, opt = build_optimizer(args, model.parameters())\n",
    "for i in range(100):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    pred = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = loss_fn(pred, data.y, data.x, data.edge_index, data.edge_attr)\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "print(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args):\n",
    "\n",
    "    print(\"Node task. test set size:\", np.sum(dataset[0]['test_mask'].numpy()))\n",
    "    print()\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    output_dim = 1 # we predict scalar potential values for each vertex\n",
    "    model = CBN(dataset.num_node_features, output_dim, dataset.num_edge_features, args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            print(f\"BATCH {batch}\")\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            print(f\"BATCH y: {batch.y.shape}\")\n",
    "            # pred = pred[batch.train_mask]\n",
    "            # label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "\n",
    "    return test_accs, losses, best_model, best_acc, test_loader\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
    "    test_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    # Note that Cora is only one graph!\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            pred = test_model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = label[mask]\n",
    "\n",
    "        if save_model_preds:\n",
    "          print (\"Saving Model Predictions for Model Type\", model_type)\n",
    "\n",
    "          data = {}\n",
    "          data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "          data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "          df = pd.DataFrame(data=data)\n",
    "          # Save locally as csv\n",
    "          df.to_csv('MinCostFlow-' + model_type + '.csv', sep=',', index=False)\n",
    "\n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
