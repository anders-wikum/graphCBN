{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from MinCostDataset import MinCostDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(1)\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = MinCostDataset(root = \"./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinCostDataset(132)\n",
      "num features: 1\n",
      "num edge features: 2\n",
      "first graph: Data(x=[9559, 1], edge_index=[2, 29682], edge_attr=[29682, 2], y=[1, 1], filename='data/raw/road_flow_01_DC_a.txt')\n"
     ]
    }
   ],
   "source": [
    "def dataset_information(dataset):\n",
    "    print(dataset)\n",
    "    print(f\"num features: {dataset.num_features}\")\n",
    "    print(f\"num edge features: {dataset.num_edge_features}\")\n",
    "    print(f\"first graph: {dataset[0]}\")\n",
    "\n",
    "dataset_information(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def train_test_validation_split(dataset, train = 0.7, validation = 0.15):\n",
    "    \"\"\"\n",
    "    Test split is 1 - train - validation\n",
    "    \"\"\"\n",
    "\n",
    "    length = dataset.len()\n",
    "    shuffled_dataset = np.arange(length)\n",
    "    np.random.shuffle(shuffled_dataset)\n",
    "\n",
    "    train_cutoff = int(train * length)\n",
    "    validation_cutoff = int((train + validation) * length)\n",
    "\n",
    "    train_data = shuffled_dataset[:train_cutoff]\n",
    "    validation_data = shuffled_dataset[train_cutoff: validation_cutoff]\n",
    "    test_data = shuffled_dataset[validation_cutoff:]\n",
    "\n",
    "    return train_data, validation_data, test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def create_split_directories(dataset, split, split_name):\n",
    "    src_folder = dataset.processed_dir\n",
    "    dst_folder = os.path.join(dataset.root, split_name)\n",
    "\n",
    "    # Remove files in case some were already present\n",
    "    if os.path.exists(dst_folder):\n",
    "        shutil.rmtree(dst_folder)\n",
    "    os.makedirs(dst_folder)\n",
    "\n",
    "    for file_id in split:\n",
    "        file_name = f\"data_{file_id}.pt\"\n",
    "        src = os.path.join(src_folder, file_name)\n",
    "        dst = os.path.join(dst_folder, file_name)\n",
    "        shutil.copyfile(src, dst)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_frac = 0.7, validation_frac = 0.15):\n",
    "    train, validation, test = train_test_validation_split(dataset, train_frac, validation_frac)\n",
    "    create_split_directories(dataset, train, \"data_train/processed\")\n",
    "    create_split_directories(dataset, test, \"data_test/processed\")\n",
    "    create_split_directories(dataset, validation, \"data_validation/processed\")\n",
    "split_dataset(dataset, validation_frac = 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CBN(torch.nn.Module):\n",
    "    #TODO cite the colab\n",
    "    def __init__(self, input_dim, output_dim, edge_feature_dim, args):\n",
    "        super(CBN, self).__init__()\n",
    "\n",
    "        hidden_dim = args.hidden_dim\n",
    "        num_layers = args.num_layers\n",
    "        dropout = args.dropout\n",
    "\n",
    "        if num_layers > 1:\n",
    "            conv_modules = [NNConv(input_dim, hidden_dim, nn.Linear(edge_feature_dim, input_dim * hidden_dim))]\n",
    "            conv_modules.extend([NNConv(hidden_dim, hidden_dim, nn.Linear(edge_feature_dim, hidden_dim * hidden_dim)) for _ in range(num_layers - 2)])\n",
    "            conv_modules.append(NNConv(hidden_dim, output_dim, nn.Linear(edge_feature_dim, hidden_dim * output_dim)))\n",
    "\n",
    "            self.convs = nn.ModuleList(conv_modules)\n",
    "        else:\n",
    "            self.convs = nn.ModuleList([NNConv(input_dim, output_dim, nn.Linear(edge_feature_dim, input_dim * output_dim))])\n",
    "\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # self.post_mp = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        # self.post_mp.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        # x = self.post_mp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def dual_value(N, p):\n",
    "        return np.sum([p[i] * N.b[i] for i in N.V]) + np.sum([N.u[e] * max(0, p[e[1]] - p[e[0]] - N.c[e]) for e in N.E])\n",
    "\n",
    "    # def loss(self, pred, label, x, edge_index, edge_attr):\n",
    "    #     # edge_attr[0] is capacity, edge_attr[1] is cost\n",
    "    #     print(pred.shape)\n",
    "    #     print(edge_index[0].shape)\n",
    "    #     print(pred[edge_index[1]].shape)\n",
    "    #     print(edge_attr[:, 1].shape)\n",
    "    #     reduced_cost = pred[edge_index[1]].squeeze() - pred[edge_index[0]].squeeze() - edge_attr[:, 1]\n",
    "    #     print(reduced_cost.shape)\n",
    "    #     return label - torch.dot(pred.squeeze(), x.squeeze()) - torch.dot(edge_attr[:, 0], F.relu(reduced_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DualLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, label, x, edge_index, edge_attr):\n",
    "        # edge_attr[0] is capacity, edge_attr[1] is cost\n",
    "        #TODO is negative for the moment but if you switch the sign before the second dot product it's always positive ._.\n",
    "        reduced_cost = pred[edge_index[1]].squeeze() - pred[edge_index[0]].squeeze() - edge_attr[:, 1]\n",
    "        print(label, torch.dot(pred.squeeze(), x.squeeze()) - torch.dot(edge_attr[:, 0], F.relu(reduced_cost)))\n",
    "        return label - torch.dot(pred.squeeze(), x.squeeze()) + torch.dot(edge_attr[:, 0], F.relu(reduced_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[944]]) tensor(-1361.0535, grad_fn=<SubBackward0>)\n",
      "loss: 2305.053466796875\n",
      "tensor([[944]]) tensor(-24056.7871, grad_fn=<SubBackward0>)\n",
      "loss: 25000.787109375\n",
      "tensor([[944]]) tensor(-687.4857, grad_fn=<SubBackward0>)\n",
      "loss: 1631.485595703125\n",
      "tensor([[944]]) tensor(-10161.3301, grad_fn=<SubBackward0>)\n",
      "loss: 11105.330078125\n",
      "tensor([[944]]) tensor(-1042.4843, grad_fn=<SubBackward0>)\n",
      "loss: 1986.484130859375\n",
      "tensor([[944]]) tensor(-141.1557, grad_fn=<SubBackward0>)\n",
      "loss: 1085.1556396484375\n",
      "tensor([[944]]) tensor(-81.2302, grad_fn=<SubBackward0>)\n",
      "loss: 1025.2301025390625\n",
      "tensor([[944]]) tensor(-2210.6299, grad_fn=<SubBackward0>)\n",
      "loss: 3154.6298828125\n",
      "tensor([[944]]) tensor(22.3147, grad_fn=<SubBackward0>)\n",
      "loss: 921.685302734375\n",
      "tensor([[944]]) tensor(76.7605, grad_fn=<SubBackward0>)\n",
      "loss: 867.2395629882812\n",
      "tensor([[944]]) tensor(103.7510, grad_fn=<SubBackward0>)\n",
      "loss: 840.2490234375\n",
      "tensor([[944]]) tensor(181.8022, grad_fn=<SubBackward0>)\n",
      "loss: 762.19775390625\n",
      "tensor([[944]]) tensor(-43.8521, grad_fn=<SubBackward0>)\n",
      "loss: 987.8521118164062\n",
      "tensor([[944]]) tensor(138.0923, grad_fn=<SubBackward0>)\n",
      "loss: 805.9076538085938\n",
      "tensor([[944]]) tensor(143.0988, grad_fn=<SubBackward0>)\n",
      "loss: 800.9012451171875\n",
      "tensor([[944]]) tensor(150.4553, grad_fn=<SubBackward0>)\n",
      "loss: 793.544677734375\n",
      "tensor([[944]]) tensor(265.6642, grad_fn=<SubBackward0>)\n",
      "loss: 678.3358154296875\n",
      "tensor([[944]]) tensor(-887.1613, grad_fn=<SubBackward0>)\n",
      "loss: 1831.161376953125\n",
      "tensor([[944]]) tensor(241.0412, grad_fn=<SubBackward0>)\n",
      "loss: 702.9588012695312\n",
      "tensor([[944]]) tensor(289.3804, grad_fn=<SubBackward0>)\n",
      "loss: 654.61962890625\n",
      "tensor([[944]]) tensor(235.5065, grad_fn=<SubBackward0>)\n",
      "loss: 708.4934692382812\n",
      "tensor([[944]]) tensor(152.9809, grad_fn=<SubBackward0>)\n",
      "loss: 791.01904296875\n",
      "tensor([[944]]) tensor(8.5215, grad_fn=<SubBackward0>)\n",
      "loss: 935.4784545898438\n",
      "tensor([[944]]) tensor(242.8238, grad_fn=<SubBackward0>)\n",
      "loss: 701.1761474609375\n",
      "tensor([[944]]) tensor(353.6347, grad_fn=<SubBackward0>)\n",
      "loss: 590.3652954101562\n",
      "tensor([[944]]) tensor(378.2504, grad_fn=<SubBackward0>)\n",
      "loss: 565.7496337890625\n",
      "tensor([[944]]) tensor(352.9767, grad_fn=<SubBackward0>)\n",
      "loss: 591.0233154296875\n",
      "tensor([[944]]) tensor(464.5139, grad_fn=<SubBackward0>)\n",
      "loss: 479.4860534667969\n",
      "tensor([[944]]) tensor(384.1589, grad_fn=<SubBackward0>)\n",
      "loss: 559.841064453125\n",
      "tensor([[944]]) tensor(381.9768, grad_fn=<SubBackward0>)\n",
      "loss: 562.023193359375\n",
      "tensor([[944]]) tensor(345.3820, grad_fn=<SubBackward0>)\n",
      "loss: 598.6180419921875\n",
      "tensor([[944]]) tensor(391.5259, grad_fn=<SubBackward0>)\n",
      "loss: 552.4740600585938\n",
      "tensor([[944]]) tensor(486.9698, grad_fn=<SubBackward0>)\n",
      "loss: 457.0301818847656\n",
      "tensor([[944]]) tensor(568.9890, grad_fn=<SubBackward0>)\n",
      "loss: 375.01104736328125\n",
      "tensor([[944]]) tensor(595.3710, grad_fn=<SubBackward0>)\n",
      "loss: 348.62896728515625\n",
      "tensor([[944]]) tensor(544.5524, grad_fn=<SubBackward0>)\n",
      "loss: 399.44757080078125\n",
      "tensor([[944]]) tensor(433.5739, grad_fn=<SubBackward0>)\n",
      "loss: 510.4261169433594\n",
      "tensor([[944]]) tensor(479.9240, grad_fn=<SubBackward0>)\n",
      "loss: 464.0759582519531\n",
      "tensor([[944]]) tensor(763.4232, grad_fn=<SubBackward0>)\n",
      "loss: 180.5767822265625\n",
      "tensor([[944]]) tensor(533.4096, grad_fn=<SubBackward0>)\n",
      "loss: 410.59039306640625\n",
      "tensor([[944]]) tensor(756.4133, grad_fn=<SubBackward0>)\n",
      "loss: 187.586669921875\n",
      "tensor([[944]]) tensor(922.7445, grad_fn=<SubBackward0>)\n",
      "loss: 21.2554931640625\n",
      "tensor([[944]]) tensor(740.8472, grad_fn=<SubBackward0>)\n",
      "loss: 203.15283203125\n",
      "tensor([[944]]) tensor(718.5684, grad_fn=<SubBackward0>)\n",
      "loss: 225.43157958984375\n",
      "tensor([[944]]) tensor(815.1243, grad_fn=<SubBackward0>)\n",
      "loss: 128.87567138671875\n",
      "tensor([[944]]) tensor(1014.5590, grad_fn=<SubBackward0>)\n",
      "loss: -70.5589599609375\n",
      "tensor([[944]]) tensor(730.5360, grad_fn=<SubBackward0>)\n",
      "loss: 213.4639892578125\n",
      "tensor([[944]]) tensor(866.7601, grad_fn=<SubBackward0>)\n",
      "loss: 77.2398681640625\n",
      "tensor([[944]]) tensor(977.5225, grad_fn=<SubBackward0>)\n",
      "loss: -33.5224609375\n",
      "tensor([[944]]) tensor(784.4321, grad_fn=<SubBackward0>)\n",
      "loss: 159.56793212890625\n",
      "tensor([[944]]) tensor(865.9558, grad_fn=<SubBackward0>)\n",
      "loss: 78.044189453125\n",
      "tensor([[944]]) tensor(1241.0559, grad_fn=<SubBackward0>)\n",
      "loss: -297.055908203125\n",
      "tensor([[944]]) tensor(826.0201, grad_fn=<SubBackward0>)\n",
      "loss: 117.9798583984375\n",
      "tensor([[944]]) tensor(1143.3246, grad_fn=<SubBackward0>)\n",
      "loss: -199.3245849609375\n",
      "tensor([[944]]) tensor(1080.1520, grad_fn=<SubBackward0>)\n",
      "loss: -136.1519775390625\n",
      "tensor([[944]]) tensor(1035.5537, grad_fn=<SubBackward0>)\n",
      "loss: -91.5537109375\n",
      "tensor([[944]]) tensor(1019.7376, grad_fn=<SubBackward0>)\n",
      "loss: -75.73760986328125\n",
      "tensor([[944]]) tensor(1118.2274, grad_fn=<SubBackward0>)\n",
      "loss: -174.2274169921875\n",
      "tensor([[944]]) tensor(1088.0674, grad_fn=<SubBackward0>)\n",
      "loss: -144.0673828125\n",
      "tensor([[944]]) tensor(1102.9221, grad_fn=<SubBackward0>)\n",
      "loss: -158.922119140625\n",
      "tensor([[944]]) tensor(1353.1595, grad_fn=<SubBackward0>)\n",
      "loss: -409.1595458984375\n",
      "tensor([[944]]) tensor(1050.2637, grad_fn=<SubBackward0>)\n",
      "loss: -106.263671875\n",
      "tensor([[944]]) tensor(1415.4189, grad_fn=<SubBackward0>)\n",
      "loss: -471.4189453125\n",
      "tensor([[944]]) tensor(1150.1526, grad_fn=<SubBackward0>)\n",
      "loss: -206.152587890625\n",
      "tensor([[944]]) tensor(1465.2205, grad_fn=<SubBackward0>)\n",
      "loss: -521.220458984375\n",
      "tensor([[944]]) tensor(1380.3237, grad_fn=<SubBackward0>)\n",
      "loss: -436.32373046875\n",
      "tensor([[944]]) tensor(1302.5602, grad_fn=<SubBackward0>)\n",
      "loss: -358.5601806640625\n",
      "tensor([[944]]) tensor(1338.4207, grad_fn=<SubBackward0>)\n",
      "loss: -394.420654296875\n",
      "tensor([[944]]) tensor(1274.5908, grad_fn=<SubBackward0>)\n",
      "loss: -330.5908203125\n",
      "tensor([[944]]) tensor(1564.2557, grad_fn=<SubBackward0>)\n",
      "loss: -620.2557373046875\n",
      "tensor([[944]]) tensor(1652.3945, grad_fn=<SubBackward0>)\n",
      "loss: -708.39453125\n",
      "tensor([[944]]) tensor(1920.7894, grad_fn=<SubBackward0>)\n",
      "loss: -976.7894287109375\n",
      "tensor([[944]]) tensor(1716.2625, grad_fn=<SubBackward0>)\n",
      "loss: -772.262451171875\n",
      "tensor([[944]]) tensor(1841.1200, grad_fn=<SubBackward0>)\n",
      "loss: -897.1199951171875\n",
      "tensor([[944]]) tensor(2090.2969, grad_fn=<SubBackward0>)\n",
      "loss: -1146.296875\n",
      "tensor([[944]]) tensor(1805.1292, grad_fn=<SubBackward0>)\n",
      "loss: -861.129150390625\n",
      "tensor([[944]]) tensor(1973.5425, grad_fn=<SubBackward0>)\n",
      "loss: -1029.54248046875\n",
      "tensor([[944]]) tensor(2294.5247, grad_fn=<SubBackward0>)\n",
      "loss: -1350.524658203125\n",
      "tensor([[944]]) tensor(2078.3223, grad_fn=<SubBackward0>)\n",
      "loss: -1134.322265625\n",
      "tensor([[944]]) tensor(1960.4890, grad_fn=<SubBackward0>)\n",
      "loss: -1016.489013671875\n",
      "tensor([[944]]) tensor(1456.9967, grad_fn=<SubBackward0>)\n",
      "loss: -512.9967041015625\n",
      "tensor([[944]]) tensor(2204.6321, grad_fn=<SubBackward0>)\n",
      "loss: -1260.632080078125\n",
      "tensor([[944]]) tensor(2044.0106, grad_fn=<SubBackward0>)\n",
      "loss: -1100.0106201171875\n",
      "tensor([[944]]) tensor(2923.8977, grad_fn=<SubBackward0>)\n",
      "loss: -1979.897705078125\n",
      "tensor([[944]]) tensor(2143.8982, grad_fn=<SubBackward0>)\n",
      "loss: -1199.898193359375\n",
      "tensor([[944]]) tensor(2640.4836, grad_fn=<SubBackward0>)\n",
      "loss: -1696.483642578125\n",
      "tensor([[944]]) tensor(2537.6597, grad_fn=<SubBackward0>)\n",
      "loss: -1593.65966796875\n",
      "tensor([[944]]) tensor(2474.6965, grad_fn=<SubBackward0>)\n",
      "loss: -1530.696533203125\n",
      "tensor([[944]]) tensor(2413.2744, grad_fn=<SubBackward0>)\n",
      "loss: -1469.2744140625\n",
      "tensor([[944]]) tensor(2066.7205, grad_fn=<SubBackward0>)\n",
      "loss: -1122.720458984375\n",
      "tensor([[944]]) tensor(2565.4583, grad_fn=<SubBackward0>)\n",
      "loss: -1621.458251953125\n",
      "tensor([[944]]) tensor(2531.0657, grad_fn=<SubBackward0>)\n",
      "loss: -1587.065673828125\n",
      "tensor([[944]]) tensor(2807.2942, grad_fn=<SubBackward0>)\n",
      "loss: -1863.294189453125\n",
      "tensor([[944]]) tensor(2922.3193, grad_fn=<SubBackward0>)\n",
      "loss: -1978.3193359375\n",
      "tensor([[944]]) tensor(3036.2534, grad_fn=<SubBackward0>)\n",
      "loss: -2092.25341796875\n",
      "tensor([[944]]) tensor(3927.1663, grad_fn=<SubBackward0>)\n",
      "loss: -2983.166259765625\n",
      "tensor([[944]]) tensor(2872.9756, grad_fn=<SubBackward0>)\n",
      "loss: -1928.9755859375\n",
      "tensor([[944]]) tensor(3987.0376, grad_fn=<SubBackward0>)\n",
      "loss: -3043.03759765625\n",
      "tensor([[944]]) tensor(3075.0391, grad_fn=<SubBackward0>)\n",
      "loss: -2131.0390625\n",
      "tensor([[944]]) tensor(3526.2295, grad_fn=<SubBackward0>)\n",
      "loss: -2582.2294921875\n",
      "tensor([[944]]) tensor(3475.2993, grad_fn=<SubBackward0>)\n",
      "loss: -2531.29931640625\n",
      "tensor([[944]]) tensor(3271.6924, grad_fn=<SubBackward0>)\n",
      "loss: -2327.6923828125\n",
      "tensor([[944]]) tensor(3574.1194, grad_fn=<SubBackward0>)\n",
      "loss: -2630.119384765625\n",
      "tensor([[944]]) tensor(2907.0112, grad_fn=<SubBackward0>)\n",
      "loss: -1963.01123046875\n",
      "tensor([[944]]) tensor(3861.3269, grad_fn=<SubBackward0>)\n",
      "loss: -2917.326904296875\n",
      "tensor([[944]]) tensor(4261.0625, grad_fn=<SubBackward0>)\n",
      "loss: -3317.0625\n",
      "tensor([[944]]) tensor(3854.2356, grad_fn=<SubBackward0>)\n",
      "loss: -2910.235595703125\n",
      "tensor([[944]]) tensor(4268.2002, grad_fn=<SubBackward0>)\n",
      "loss: -3324.2001953125\n",
      "tensor([[944]]) tensor(4101.9507, grad_fn=<SubBackward0>)\n",
      "loss: -3157.95068359375\n",
      "tensor([[944]]) tensor(4342.4404, grad_fn=<SubBackward0>)\n",
      "loss: -3398.4404296875\n",
      "tensor([[944]]) tensor(4475.0259, grad_fn=<SubBackward0>)\n",
      "loss: -3531.02587890625\n",
      "tensor([[944]]) tensor(3943.4404, grad_fn=<SubBackward0>)\n",
      "loss: -2999.4404296875\n",
      "tensor([[944]]) tensor(4567.4883, grad_fn=<SubBackward0>)\n",
      "loss: -3623.48828125\n",
      "tensor([[944]]) tensor(3776.0977, grad_fn=<SubBackward0>)\n",
      "loss: -2832.09765625\n",
      "tensor([[944]]) tensor(4871.9248, grad_fn=<SubBackward0>)\n",
      "loss: -3927.9248046875\n",
      "tensor([[944]]) tensor(5584.6104, grad_fn=<SubBackward0>)\n",
      "loss: -4640.6103515625\n",
      "tensor([[944]]) tensor(5572.9316, grad_fn=<SubBackward0>)\n",
      "loss: -4628.931640625\n",
      "tensor([[944]]) tensor(5261.6660, grad_fn=<SubBackward0>)\n",
      "loss: -4317.666015625\n",
      "tensor([[944]]) tensor(4180.2065, grad_fn=<SubBackward0>)\n",
      "loss: -3236.20654296875\n",
      "tensor([[944]]) tensor(5018.6445, grad_fn=<SubBackward0>)\n",
      "loss: -4074.64453125\n",
      "tensor([[944]]) tensor(3899.8464, grad_fn=<SubBackward0>)\n",
      "loss: -2955.846435546875\n",
      "tensor([[944]]) tensor(5484.3438, grad_fn=<SubBackward0>)\n",
      "loss: -4540.34375\n",
      "tensor([[944]]) tensor(4130.6387, grad_fn=<SubBackward0>)\n",
      "loss: -3186.638671875\n",
      "tensor([[944]]) tensor(6660.1543, grad_fn=<SubBackward0>)\n",
      "loss: -5716.154296875\n",
      "tensor([[944]]) tensor(5324.8730, grad_fn=<SubBackward0>)\n",
      "loss: -4380.873046875\n",
      "tensor([[944]]) tensor(5733.3643, grad_fn=<SubBackward0>)\n",
      "loss: -4789.3642578125\n",
      "tensor([[944]]) tensor(6775.1406, grad_fn=<SubBackward0>)\n",
      "loss: -5831.140625\n",
      "tensor([[944]]) tensor(6045.7261, grad_fn=<SubBackward0>)\n",
      "loss: -5101.72607421875\n",
      "tensor([[944]]) tensor(6469.7969, grad_fn=<SubBackward0>)\n",
      "loss: -5525.796875\n",
      "tensor([[944]]) tensor(7324.5059, grad_fn=<SubBackward0>)\n",
      "loss: -6380.505859375\n",
      "tensor([[944]]) tensor(5289.1982, grad_fn=<SubBackward0>)\n",
      "loss: -4345.1982421875\n",
      "tensor([[944]]) tensor(6488.1816, grad_fn=<SubBackward0>)\n",
      "loss: -5544.181640625\n",
      "tensor([[944]]) tensor(4914.0830, grad_fn=<SubBackward0>)\n",
      "loss: -3970.0830078125\n",
      "tensor([[944]]) tensor(6948.3232, grad_fn=<SubBackward0>)\n",
      "loss: -6004.3232421875\n",
      "tensor([[944]]) tensor(7591.1802, grad_fn=<SubBackward0>)\n",
      "loss: -6647.18017578125\n",
      "tensor([[944]]) tensor(6895.7510, grad_fn=<SubBackward0>)\n",
      "loss: -5951.7509765625\n",
      "tensor([[944]]) tensor(7696.6719, grad_fn=<SubBackward0>)\n",
      "loss: -6752.671875\n",
      "tensor([[944]]) tensor(7186.2754, grad_fn=<SubBackward0>)\n",
      "loss: -6242.275390625\n",
      "tensor([[944]]) tensor(7351.6602, grad_fn=<SubBackward0>)\n",
      "loss: -6407.66015625\n",
      "tensor([[944]]) tensor(7778.2393, grad_fn=<SubBackward0>)\n",
      "loss: -6834.2392578125\n",
      "tensor([[944]]) tensor(8093.7900, grad_fn=<SubBackward0>)\n",
      "loss: -7149.7900390625\n",
      "tensor([[944]]) tensor(6228.1016, grad_fn=<SubBackward0>)\n",
      "loss: -5284.1015625\n",
      "tensor([[944]]) tensor(7950.6143, grad_fn=<SubBackward0>)\n",
      "loss: -7006.6142578125\n",
      "tensor([[944]]) tensor(6887.6797, grad_fn=<SubBackward0>)\n",
      "loss: -5943.6796875\n",
      "tensor([[944]]) tensor(9038.0273, grad_fn=<SubBackward0>)\n",
      "loss: -8094.02783203125\n",
      "tensor([[944]]) tensor(7988.3140, grad_fn=<SubBackward0>)\n",
      "loss: -7044.31396484375\n",
      "tensor([[944]]) tensor(8047.6665, grad_fn=<SubBackward0>)\n",
      "loss: -7103.66650390625\n",
      "tensor([[944]]) tensor(9931.9873, grad_fn=<SubBackward0>)\n",
      "loss: -8987.9873046875\n",
      "tensor([[944]]) tensor(7470.7622, grad_fn=<SubBackward0>)\n",
      "loss: -6526.76220703125\n",
      "tensor([[944]]) tensor(9584.7969, grad_fn=<SubBackward0>)\n",
      "loss: -8640.796875\n",
      "tensor([[944]]) tensor(8145.9834, grad_fn=<SubBackward0>)\n",
      "loss: -7201.9833984375\n",
      "tensor([[944]]) tensor(7577.8652, grad_fn=<SubBackward0>)\n",
      "loss: -6633.865234375\n",
      "tensor([[944]]) tensor(7648.1108, grad_fn=<SubBackward0>)\n",
      "loss: -6704.11083984375\n",
      "tensor([[944]]) tensor(10243.1523, grad_fn=<SubBackward0>)\n",
      "loss: -9299.15234375\n",
      "tensor([[944]]) tensor(11848.0645, grad_fn=<SubBackward0>)\n",
      "loss: -10904.064453125\n",
      "tensor([[944]]) tensor(10907.7461, grad_fn=<SubBackward0>)\n",
      "loss: -9963.74609375\n",
      "tensor([[944]]) tensor(11318.6436, grad_fn=<SubBackward0>)\n",
      "loss: -10374.6435546875\n",
      "tensor([[944]]) tensor(10939.2451, grad_fn=<SubBackward0>)\n",
      "loss: -9995.2451171875\n",
      "tensor([[944]]) tensor(10226.6523, grad_fn=<SubBackward0>)\n",
      "loss: -9282.65234375\n",
      "tensor([[944]]) tensor(10692.1875, grad_fn=<SubBackward0>)\n",
      "loss: -9748.1875\n",
      "tensor([[944]]) tensor(8172.0596, grad_fn=<SubBackward0>)\n",
      "loss: -7228.0595703125\n",
      "tensor([[944]]) tensor(11414.8350, grad_fn=<SubBackward0>)\n",
      "loss: -10470.8349609375\n",
      "tensor([[944]]) tensor(10836.3574, grad_fn=<SubBackward0>)\n",
      "loss: -9892.357421875\n",
      "tensor([[944]]) tensor(11333.6152, grad_fn=<SubBackward0>)\n",
      "loss: -10389.615234375\n",
      "tensor([[944]]) tensor(10621.9551, grad_fn=<SubBackward0>)\n",
      "loss: -9677.955078125\n",
      "tensor([[944]]) tensor(10773.8213, grad_fn=<SubBackward0>)\n",
      "loss: -9829.8212890625\n",
      "tensor([[944]]) tensor(13828.2754, grad_fn=<SubBackward0>)\n",
      "loss: -12884.275390625\n",
      "tensor([[944]]) tensor(13134.6357, grad_fn=<SubBackward0>)\n",
      "loss: -12190.6357421875\n",
      "tensor([[944]]) tensor(15726.5215, grad_fn=<SubBackward0>)\n",
      "loss: -14782.521484375\n",
      "tensor([[944]]) tensor(13756.5947, grad_fn=<SubBackward0>)\n",
      "loss: -12812.5947265625\n",
      "tensor([[944]]) tensor(11278.9805, grad_fn=<SubBackward0>)\n",
      "loss: -10334.98046875\n",
      "tensor([[944]]) tensor(14886.1113, grad_fn=<SubBackward0>)\n",
      "loss: -13942.111328125\n",
      "tensor([[944]]) tensor(13644.4590, grad_fn=<SubBackward0>)\n",
      "loss: -12700.458984375\n",
      "tensor([[944]]) tensor(14352.9531, grad_fn=<SubBackward0>)\n",
      "loss: -13408.953125\n",
      "tensor([[944]]) tensor(13933.3418, grad_fn=<SubBackward0>)\n",
      "loss: -12989.341796875\n",
      "tensor([[944]]) tensor(15267.0352, grad_fn=<SubBackward0>)\n",
      "loss: -14323.03515625\n",
      "tensor([[944]]) tensor(13594.4727, grad_fn=<SubBackward0>)\n",
      "loss: -12650.47265625\n",
      "tensor([[944]]) tensor(15815.4922, grad_fn=<SubBackward0>)\n",
      "loss: -14871.4921875\n",
      "tensor([[944]]) tensor(15094.7998, grad_fn=<SubBackward0>)\n",
      "loss: -14150.7998046875\n",
      "tensor([[944]]) tensor(17345.0898, grad_fn=<SubBackward0>)\n",
      "loss: -16401.08984375\n",
      "tensor([[944]]) tensor(18027.8535, grad_fn=<SubBackward0>)\n",
      "loss: -17083.853515625\n",
      "tensor([[944]]) tensor(16413.5781, grad_fn=<SubBackward0>)\n",
      "loss: -15469.578125\n",
      "tensor([[944]]) tensor(15261.3926, grad_fn=<SubBackward0>)\n",
      "loss: -14317.392578125\n",
      "tensor([[944]]) tensor(16840.9980, grad_fn=<SubBackward0>)\n",
      "loss: -15896.998046875\n",
      "tensor([[944]]) tensor(15767.9102, grad_fn=<SubBackward0>)\n",
      "loss: -14823.91015625\n",
      "tensor([[944]]) tensor(16050.9180, grad_fn=<SubBackward0>)\n",
      "loss: -15106.91796875\n",
      "tensor([[944]]) tensor(17324.7949, grad_fn=<SubBackward0>)\n",
      "loss: -16380.794921875\n",
      "tensor([[944]]) tensor(20403.7188, grad_fn=<SubBackward0>)\n",
      "loss: -19459.71875\n",
      "tensor([[944]]) tensor(19268.7305, grad_fn=<SubBackward0>)\n",
      "loss: -18324.73046875\n",
      "tensor([[944]]) tensor(20941.9023, grad_fn=<SubBackward0>)\n",
      "loss: -19997.90234375\n",
      "tensor([[944]]) tensor(17596.7754, grad_fn=<SubBackward0>)\n",
      "loss: -16652.775390625\n",
      "tensor([[944]]) tensor(17384.4043, grad_fn=<SubBackward0>)\n",
      "loss: -16440.404296875\n",
      "tensor([[944]]) tensor(21066.6816, grad_fn=<SubBackward0>)\n",
      "loss: -20122.681640625\n",
      "tensor([[944]]) tensor(20448.9492, grad_fn=<SubBackward0>)\n",
      "loss: -19504.94921875\n",
      "tensor([[944]]) tensor(21947.0273, grad_fn=<SubBackward0>)\n",
      "loss: -21003.02734375\n",
      "tensor([[944]]) tensor(20934.5625, grad_fn=<SubBackward0>)\n",
      "loss: -19990.5625\n",
      "tensor([[944]]) tensor(22441.1699, grad_fn=<SubBackward0>)\n",
      "loss: -21497.169921875\n",
      "tensor([[944]]) tensor(22444.7109, grad_fn=<SubBackward0>)\n",
      "loss: -21500.7109375\n",
      "tensor([[944]]) tensor(23408.3125, grad_fn=<SubBackward0>)\n",
      "loss: -22464.3125\n",
      "tensor([[944]]) tensor(24240.4473, grad_fn=<SubBackward0>)\n",
      "loss: -23296.447265625\n",
      "tensor([[944]]) tensor(18177.1660, grad_fn=<SubBackward0>)\n",
      "loss: -17233.166015625\n",
      "tensor([[944]]) tensor(18676.2656, grad_fn=<SubBackward0>)\n",
      "loss: -17732.265625\n",
      "tensor([[944]]) tensor(24554.7227, grad_fn=<SubBackward0>)\n",
      "loss: -23610.72265625\n",
      "tensor([[944]]) tensor(22314.0020, grad_fn=<SubBackward0>)\n",
      "loss: -21370.001953125\n",
      "tensor([[944]]) tensor(21769.9922, grad_fn=<SubBackward0>)\n",
      "loss: -20825.9921875\n",
      "tensor([[944]]) tensor(20155.9961, grad_fn=<SubBackward0>)\n",
      "loss: -19211.99609375\n",
      "tensor([[944]]) tensor(21386.7070, grad_fn=<SubBackward0>)\n",
      "loss: -20442.70703125\n",
      "tensor([[944]]) tensor(24041.5273, grad_fn=<SubBackward0>)\n",
      "loss: -23097.52734375\n",
      "tensor([[944]]) tensor(25613.3672, grad_fn=<SubBackward0>)\n",
      "loss: -24669.3671875\n",
      "tensor([[944]]) tensor(24254.3398, grad_fn=<SubBackward0>)\n",
      "loss: -23310.33984375\n",
      "tensor([[944]]) tensor(26426.7070, grad_fn=<SubBackward0>)\n",
      "loss: -25482.70703125\n",
      "tensor([[944]]) tensor(23511.8867, grad_fn=<SubBackward0>)\n",
      "loss: -22567.88671875\n",
      "tensor([[944]]) tensor(27143.4531, grad_fn=<SubBackward0>)\n",
      "loss: -26199.453125\n",
      "tensor([[944]]) tensor(30081.5098, grad_fn=<SubBackward0>)\n",
      "loss: -29137.509765625\n",
      "tensor([[944]]) tensor(32239.4375, grad_fn=<SubBackward0>)\n",
      "loss: -31295.4375\n",
      "tensor([[944]]) tensor(29113.6426, grad_fn=<SubBackward0>)\n",
      "loss: -28169.642578125\n",
      "tensor([[944]]) tensor(29718.7910, grad_fn=<SubBackward0>)\n",
      "loss: -28774.791015625\n",
      "tensor([[944]]) tensor(34353.6016, grad_fn=<SubBackward0>)\n",
      "loss: -33409.6015625\n",
      "tensor([[944]]) tensor(30363.8047, grad_fn=<SubBackward0>)\n",
      "loss: -29419.8046875\n",
      "tensor([[944]]) tensor(28643.1660, grad_fn=<SubBackward0>)\n",
      "loss: -27699.166015625\n",
      "tensor([[944]]) tensor(33156.0859, grad_fn=<SubBackward0>)\n",
      "loss: -32212.0859375\n",
      "tensor([[944]]) tensor(35135.9609, grad_fn=<SubBackward0>)\n",
      "loss: -34191.9609375\n",
      "tensor([[944]]) tensor(27637.6699, grad_fn=<SubBackward0>)\n",
      "loss: -26693.669921875\n",
      "tensor([[944]]) tensor(34712.3750, grad_fn=<SubBackward0>)\n",
      "loss: -33768.375\n",
      "tensor([[944]]) tensor(31796.8828, grad_fn=<SubBackward0>)\n",
      "loss: -30852.8828125\n",
      "tensor([[944]]) tensor(37335.3984, grad_fn=<SubBackward0>)\n",
      "loss: -36391.3984375\n",
      "tensor([[944]]) tensor(34143.6953, grad_fn=<SubBackward0>)\n",
      "loss: -33199.6953125\n",
      "tensor([[944]]) tensor(32745.9375, grad_fn=<SubBackward0>)\n",
      "loss: -31801.9375\n",
      "tensor([[944]]) tensor(30620.6602, grad_fn=<SubBackward0>)\n",
      "loss: -29676.66015625\n",
      "tensor([[944]]) tensor(35997.4023, grad_fn=<SubBackward0>)\n",
      "loss: -35053.40234375\n",
      "tensor([[944]]) tensor(29443.4277, grad_fn=<SubBackward0>)\n",
      "loss: -28499.427734375\n",
      "tensor([[944]]) tensor(33484.5547, grad_fn=<SubBackward0>)\n",
      "loss: -32540.5546875\n",
      "tensor([[944]]) tensor(36444.3945, grad_fn=<SubBackward0>)\n",
      "loss: -35500.39453125\n",
      "tensor([[944]]) tensor(38348.3516, grad_fn=<SubBackward0>)\n",
      "loss: -37404.3515625\n",
      "tensor([[944]]) tensor(39061.8594, grad_fn=<SubBackward0>)\n",
      "loss: -38117.859375\n",
      "tensor([[944]]) tensor(36827.5117, grad_fn=<SubBackward0>)\n",
      "loss: -35883.51171875\n",
      "tensor([[944]]) tensor(45756.9648, grad_fn=<SubBackward0>)\n",
      "loss: -44812.96484375\n",
      "tensor([[944]]) tensor(44873.9688, grad_fn=<SubBackward0>)\n",
      "loss: -43929.96875\n",
      "tensor([[944]]) tensor(36987.5000, grad_fn=<SubBackward0>)\n",
      "loss: -36043.5\n",
      "tensor([[944]]) tensor(39793.4219, grad_fn=<SubBackward0>)\n",
      "loss: -38849.421875\n",
      "tensor([[944]]) tensor(43636.5117, grad_fn=<SubBackward0>)\n",
      "loss: -42692.51171875\n",
      "tensor([[944]]) tensor(42605.9258, grad_fn=<SubBackward0>)\n",
      "loss: -41661.92578125\n",
      "tensor([[944]]) tensor(46539.7109, grad_fn=<SubBackward0>)\n",
      "loss: -45595.7109375\n",
      "tensor([[944]]) tensor(41442.5664, grad_fn=<SubBackward0>)\n",
      "loss: -40498.56640625\n",
      "tensor([[944]]) tensor(43506.5664, grad_fn=<SubBackward0>)\n",
      "loss: -42562.56640625\n",
      "tensor([[944]]) tensor(42349.6641, grad_fn=<SubBackward0>)\n",
      "loss: -41405.6640625\n",
      "tensor([[944]]) tensor(40559.4180, grad_fn=<SubBackward0>)\n",
      "loss: -39615.41796875\n",
      "tensor([[944]]) tensor(43408.7656, grad_fn=<SubBackward0>)\n",
      "loss: -42464.765625\n",
      "tensor([[944]]) tensor(52986.4297, grad_fn=<SubBackward0>)\n",
      "loss: -52042.4296875\n",
      "tensor([[944]]) tensor(45592.6406, grad_fn=<SubBackward0>)\n",
      "loss: -44648.640625\n",
      "tensor([[416.2497],\n",
      "        [498.9642],\n",
      "        [469.0821],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000],\n",
      "        [  0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#TODO handle batch size > 1\n",
    "args = {\n",
    "    'num_layers': 5, \n",
    "    'batch_size': 1,\n",
    "    'hidden_dim': 64, \n",
    "    'dropout': 0.5, \n",
    "    'epochs': 500, \n",
    "    'opt': 'adam', \n",
    "    'opt_scheduler': 'none', \n",
    "    'opt_restart': 0, \n",
    "    'weight_decay': 5e-3, \n",
    "    'lr': 0.01\n",
    "}\n",
    "args = objectview(args)\n",
    "model = CBN(1, 1, 2, args)\n",
    "loss_fn = DualLoss()\n",
    "data = dataset[40]\n",
    "\n",
    "\n",
    "scheduler, opt = build_optimizer(args, model.parameters())\n",
    "for i in range(250):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    pred = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = loss_fn(pred, data.y, data.x, data.edge_index, data.edge_attr)\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([112.58583 ,  89.155945,  82.62709 ,   0.      ,   0.      ,\n         0.      ,   0.      ,   0.      ,   0.      ,   0.      ,\n         0.      ,   0.      ,   0.      ,   0.      ,   0.      ,\n         0.      ,   0.      ,   0.      ,   0.      ,   0.      ],\n      dtype=float32)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pred.detach().numpy().flatten()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loader = DataLoader(MinCostDataset(root = \"./data/data_train\"), batch_size = args.batch_size, shuffle = True)\n",
    "test_loader = DataLoader(MinCostDataset(root = \"./data/data_test\"), batch_size = args.batch_size, shuffle = True)\n",
    "# TODO also define the validation loader (and set validation fraction > 0 lol)\n",
    "\n",
    "# Output dimension is 1 since we predict scalar potential values for each vertex\n",
    "model = CBN(1, 1, 2, args)\n",
    "loss_fn = DualLoss()\n",
    "scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            print(f\"BATCH {batch}\")\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            label = batch.y\n",
    "            print(f\"BATCH y: {batch.y.shape}\")\n",
    "            # pred = pred[batch.train_mask]\n",
    "            # label = label[batch.train_mask]\n",
    "            loss = loss_fn(pred, batch.y, batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "\n",
    "    return test_accs, losses, best_model, best_acc\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
    "    # TODO handle is_validation\n",
    "    test_model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = {}\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            label = batch.y\n",
    "\n",
    "            loss = loss_fn(pred, batch.y, batch.x, batch.edge_index, batch.edge_attr)\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "            # TODO handle save_model_preds (Q: how to keep track of which original file we're working on?) inspiration in commented code below\n",
    "\n",
    "            # if save_model_preds:\n",
    "            #     print (\"Saving Model Predictions for Model Type\", model_type)\n",
    "            #\n",
    "            #     data = {}\n",
    "            #     data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "            #     data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "            #\n",
    "            #     df = pd.DataFrame(data=data)\n",
    "            #     # Save locally as csv\n",
    "            #     df.to_csv('MinCostFlow-' + model_type + '.csv', sep=',', index=False)\n",
    "\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/500 [00:00<?, ?Epochs/s]\n",
      "  0%|          | 0/92 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH DataBatch(x=[20, 1], edge_index=[2, 66], edge_attr=[66, 2], y=[1, 1], filename=[1], batch=[20], ptr=[2])\n",
      "BATCH y: torch.Size([1, 1])\n",
      "tensor([[1139]]) tensor(-170825.0938, grad_fn=<SubBackward0>)\n",
      "BATCH DataBatch(x=[9559, 1], edge_index=[2, 29682], edge_attr=[29682, 2], y=[1, 1], filename=[1], batch=[9559], ptr=[2])\n",
      "BATCH y: torch.Size([1, 1])\n",
      "tensor([[238475.]], dtype=torch.float64) tensor(-13296692., grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 2/92 [00:02<01:35,  1.06s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH DataBatch(x=[116915, 1], edge_index=[2, 262958], edge_attr=[262958, 2], y=[1, 1], filename=[1], batch=[116915], ptr=[2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/92 [00:15<11:48,  7.87s/it]\n",
      "Training:   0%|          | 0/500 [00:15<?, ?Epochs/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test_accs, losses, best_model, best_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaximum test set accuracy: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mmax\u001B[39m(test_accs)))\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMinimum loss: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mmin\u001B[39m(losses)))\n",
      "Cell \u001B[0;32mIn[52], line 42\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBATCH \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     41\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 42\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_attr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m label \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39my\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBATCH y: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[45], line 39\u001B[0m, in \u001B[0;36mCBN.forward\u001B[0;34m(self, x, edge_index, edge_attr)\u001B[0m\n\u001B[1;32m     37\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvs[i](x, edge_index, edge_attr)\n\u001B[1;32m     38\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbns[i](x)\n\u001B[0;32m---> 39\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n\u001B[1;32m     41\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m](x, edge_index, edge_attr)\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/torch/nn/functional.py:1457\u001B[0m, in \u001B[0;36mrelu\u001B[0;34m(input, inplace)\u001B[0m\n\u001B[1;32m   1455\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m   1456\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1457\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test_accs, losses, best_model, best_acc = train(args)\n",
    "\n",
    "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "# Run test for our best model to save the predictions!\n",
    "# test(test_loader, best_model, is_validation=False, save_model_preds=True, model_type=model)\n",
    "print()\n",
    "\n",
    "plt.title(dataset.name)\n",
    "plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
    "plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
