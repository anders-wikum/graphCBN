{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "import torch\n",
            "print(\"PyTorch has version {}\".format(torch.__version__))\n",
            "import torch.nn as nn\n",
            "from MinCostDataset import MinCostDataset\n",
            "import numpy as np\n",
            "import os\n",
            "import shutil\n",
            "\n",
            "import torch_geometric\n",
            "from torch_geometric.loader import DataLoader\n",
            "from torch_geometric.nn import NNConv\n",
            "import torch.nn.functional as F\n",
            "\n",
            "np.random.seed(1)\n",
            "from tqdm import tqdm\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "dataset = MinCostDataset(root = \"./data/\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "def dataset_information(dataset):\n",
            "    print(dataset)\n",
            "    print(f\"num features: {dataset.num_features}\")\n",
            "    print(f\"num edge features: {dataset.num_edge_features}\")\n",
            "    print(f\"first graph: {dataset[0]}\")\n",
            "\n",
            "dataset_information(dataset)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "def train_test_validation_split(dataset, train = 0.7, validation = 0.15):\n",
            "    \"\"\"\n",
            "    Test split is 1 - train - validation\n",
            "    \"\"\"\n",
            "\n",
            "    length = dataset.len()\n",
            "    shuffled_dataset = np.arange(length)\n",
            "    np.random.shuffle(shuffled_dataset)\n",
            "\n",
            "    train_cutoff = int(train * length)\n",
            "    validation_cutoff = int((train + validation) * length)\n",
            "\n",
            "    train_data = shuffled_dataset[:train_cutoff]\n",
            "    validation_data = shuffled_dataset[train_cutoff: validation_cutoff]\n",
            "    test_data = shuffled_dataset[validation_cutoff:]\n",
            "\n",
            "    return train_data, validation_data, test_data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "def create_split_directories(dataset, split, split_name):\n",
            "    src_folder = dataset.processed_dir\n",
            "    dst_folder = os.path.join(dataset.root, split_name)\n",
            "\n",
            "    # Remove files in case some were already present\n",
            "    if os.path.exists(dst_folder):\n",
            "        shutil.rmtree(dst_folder)\n",
            "    os.makedirs(dst_folder)\n",
            "    dst_index = 0\n",
            "    for file_id in split:\n",
            "        src_file_name = f\"data_{file_id}.pt\"\n",
            "        # The files are always expected by PyG to be ordered\n",
            "        dst_file_name = f\"data_{dst_index}.pt\"\n",
            "        src = os.path.join(src_folder, src_file_name)\n",
            "        dst = os.path.join(dst_folder, dst_file_name)\n",
            "        shutil.copyfile(src, dst)\n",
            "        dst_index += 1\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "def split_dataset(dataset, train_frac = 0.7, validation_frac = 0.15):\n",
            "    train, validation, test = train_test_validation_split(dataset, train_frac, validation_frac)\n",
            "    create_split_directories(dataset, train, \"data_train/processed\")\n",
            "    create_split_directories(dataset, test, \"data_test/processed\")\n",
            "    create_split_directories(dataset, validation, \"data_validation/processed\")\n",
            "split_dataset(dataset, validation_frac = 0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "class CBN(torch.nn.Module):\n",
            "    #TODO cite the colab\n",
            "    def __init__(self, input_dim, output_dim, edge_feature_dim, args):\n",
            "        super(CBN, self).__init__()\n",
            "\n",
            "        hidden_dim = args.hidden_dim\n",
            "        num_layers = args.num_layers\n",
            "        dropout = args.dropout\n",
            "\n",
            "        if num_layers > 1:\n",
            "            conv_modules = [NNConv(input_dim, hidden_dim, nn.Linear(edge_feature_dim, input_dim * hidden_dim))]\n",
            "            conv_modules.extend([NNConv(hidden_dim, hidden_dim, nn.Linear(edge_feature_dim, hidden_dim * hidden_dim)) for _ in range(num_layers - 2)])\n",
            "            conv_modules.append(NNConv(hidden_dim, output_dim, nn.Linear(edge_feature_dim, hidden_dim * output_dim)))\n",
            "\n",
            "            self.convs = nn.ModuleList(conv_modules)\n",
            "        else:\n",
            "            self.convs = nn.ModuleList([NNConv(input_dim, output_dim, nn.Linear(edge_feature_dim, input_dim * output_dim))])\n",
            "\n",
            "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
            "\n",
            "        # self.post_mp = nn.Linear(hidden_dim, 1)\n",
            "\n",
            "        self.num_layers = num_layers\n",
            "\n",
            "        # Probability of an element getting zeroed\n",
            "        self.dropout = dropout\n",
            "\n",
            "    def reset_parameters(self):\n",
            "        for conv in self.convs:\n",
            "            conv.reset_parameters()\n",
            "        for bn in self.bns:\n",
            "            bn.reset_parameters()\n",
            "        # self.post_mp.reset_parameters()\n",
            "\n",
            "    def forward(self, x, edge_index, edge_attr):\n",
            "        for i in range(self.num_layers - 1):\n",
            "            x = self.convs[i](x, edge_index, edge_attr)\n",
            "            x = self.bns[i](x)\n",
            "            x = F.relu(x)\n",
            "            x = F.dropout(x, self.dropout, self.training)\n",
            "        x = self.convs[-1](x, edge_index, edge_attr)\n",
            "        # x = F.relu(x)\n",
            "        # x = self.post_mp(x)\n",
            "\n",
            "        return x\n",
            "\n",
            "    def dual_value(pred, x, edge_attr, edge_index):\n",
            "        reduced_cost = edge_attr[:, 1] + pred[edge_index[0]].squeeze() - pred[edge_index[1]].squeeze()\n",
            "        return -torch.dot(pred.squeeze(), x.squeeze()) - torch.dot(edge_attr[:, 0], F.relu(-reduced_cost))\n",
            "\n",
            "    # def loss(self, pred, label, x, edge_index, edge_attr):\n",
            "    #     # edge_attr[0] is capacity, edge_attr[1] is cost\n",
            "    #     print(pred.shape)\n",
            "    #     print(edge_index[0].shape)\n",
            "    #     print(pred[edge_index[1]].shape)\n",
            "    #     print(edge_attr[:, 1].shape)\n",
            "    #     reduced_cost = pred[edge_index[1]].squeeze() - pred[edge_index[0]].squeeze() - edge_attr[:, 1]\n",
            "    #     print(reduced_cost.shape)\n",
            "    #     return label - torch.dot(pred.squeeze(), x.squeeze()) - torch.dot(edge_attr[:, 0], F.relu(reduced_cost))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "class DualLoss(nn.Module):\n",
            "    def __init__(self):\n",
            "        super(DualLoss, self).__init__()\n",
            "\n",
            "    def forward(self, pred, label, x, edge_index, edge_attr):\n",
            "        reduced_cost = edge_attr[:, 1] + pred[edge_index[0]].squeeze() - pred[edge_index[1]].squeeze()\n",
            "        potentials = label[:, 1]\n",
            "        opt = label[0, 0]\n",
            "        opt_loss = (opt + torch.dot(pred.squeeze(), x.squeeze()) + torch.dot(edge_attr[:, 0], F.relu(-reduced_cost))) / opt\n",
            "        potential_loss = torch.linalg.vector_norm(pred-potentials, ord=2) \n",
            "        loss = opt_loss + 0.000005 * potential_loss\n",
            "        print(f\"opt_loss: {opt_loss}, 'pot_loss: {0.000005 * potential_loss}\")\n",
            "        return loss\n",
            "        # edge_attr[0] is capacity, edge_attr[1] is cost\n",
            "        reduced_cost = pred[edge_index[1]].squeeze() - pred[edge_index[0]].squeeze() - edge_attr[:, 1]\n",
            "       \n",
            "        reg = 0.0005 * sum(F.relu(-reduced_cost))\n",
            "       \n",
            "        return loss + reg"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "class objectview(object):\n",
            "    def __init__(self, d):\n",
            "        self.__dict__ = d"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "import torch.optim as optim\n",
            "\n",
            "def build_optimizer(args, params):\n",
            "    weight_decay = args.weight_decay\n",
            "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
            "    if args.opt == 'adam':\n",
            "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
            "    elif args.opt == 'sgd':\n",
            "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
            "    elif args.opt == 'rmsprop':\n",
            "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
            "    elif args.opt == 'adagrad':\n",
            "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
            "    if args.opt_scheduler == 'none':\n",
            "        return None, optimizer\n",
            "    elif args.opt_scheduler == 'step':\n",
            "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
            "    elif args.opt_scheduler == 'cos':\n",
            "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
            "    return scheduler, optimizer"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "dataset.process()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "\n",
            "#TODO handle batch size > 1\n",
            "args = {\n",
            "    'num_layers': 3,\n",
            "    'batch_size': 1,\n",
            "    'hidden_dim': 32,\n",
            "    'dropout': 0,\n",
            "    'epochs': 500,\n",
            "    'opt': 'adam',\n",
            "    'opt_scheduler': 'none',\n",
            "    'opt_restart': 0,\n",
            "    'weight_decay': 5e-3,\n",
            "    'lr': 0.001\n",
            "}\n",
            "args = objectview(args)\n",
            "model = CBN(1, 1, 2, args)\n",
            "loss_fn = DualLoss()\n",
            "data = dataset[0]\n",
            "\n",
            "\n",
            "scheduler, opt = build_optimizer(args, model.parameters())\n",
            "\n",
            "for i in range(7500):\n",
            "    model.train()\n",
            "    opt.zero_grad()\n",
            "    pred = model(data.x, data.edge_index, data.edge_attr)\n",
            "    loss = loss_fn(pred, data.y, data.x, data.edge_index, data.edge_attr)\n",
            "    print(f\"loss: {loss.item()}\")\n",
            "    loss.backward()\n",
            "    opt.step()\n",
            "\n",
            "print(data)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "reduced_cost = -pred[data.edge_index[1]].squeeze() + pred[data.edge_index[0]].squeeze() + data.edge_attr[:, 1]\n",
            "reduced_cost = reduced_cost.detach().numpy().flatten()\n",
            "threshold = np.quantile(reduced_cost[reduced_cost < 0], 0.1)\n",
            "neg_edges = list(zip(*data.edge_index[:, reduced_cost < threshold].numpy()))\n",
            "pos_edges = list(zip(*data.edge_index[:, reduced_cost > threshold].numpy()))\n",
            "edges = list(zip(*data.edge_index.numpy()))\n",
            "costs = dict(zip(edges, list(data.edge_attr[:, 1].numpy())))\n",
            "caps = dict(zip(edges, list(data.edge_attr[:, 0].numpy())))\n",
            "caps = np.array([caps[(i,j)] for (i,j) in pos_edges])\n",
            "\n",
            "import cvxpy as cp\n",
            "N = data.x.size(0)\n",
            "y = cp.Variable(N)\n",
            "t = cp.Variable(len(neg_edges))\n",
            "s = cp.Variable(len(pos_edges))\n",
            "\n",
            "prob = cp.Problem(\n",
            "    cp.Minimize(10000*cp.sum(-cp.minimum(s, 0)) + 10*cp.norm(s.T @ caps, 1)),\n",
            "    [\n",
            "      *[costs[neg_edges[i]] + y[neg_edges[i][0]] - y[neg_edges[i][1]] <= t[i] for i in range(len(neg_edges))],\n",
            "      *[costs[pos_edges[j]] + y[pos_edges[j][0]] - y[pos_edges[j][1]] == s[j] for j in range(len(pos_edges))],\n",
            "      t <= -0.01\n",
            "    ]\n",
            ")\n",
            "\n",
            "prob.solve()\n",
            "\n",
            "y.value"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "false_pos = np.array([round(costs[pos_edges[j]] + y.value[pos_edges[j][0]] - y.value[pos_edges[j][1]], 4) for j in range(len(pos_edges))])\n",
            "print(len(false_pos[false_pos < 0]))\n",
            "print(len(false_pos[false_pos == 0]))\n",
            "print(len(false_pos))\n",
            "#print(list(zip(pos_edges, false_pos)))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "true_pos = np.array([round(costs[neg_edges[j]] + y.value[neg_edges[j][0]] - y.value[neg_edges[j][1]], 4) for j in range(len(neg_edges))])\n",
            "print(len(true_pos))\n",
            "print(len(true_pos[true_pos < 0]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "import time\n",
            "\n",
            "import networkx as nx\n",
            "import numpy as np\n",
            "import torch\n",
            "import torch.optim as optim\n",
            "from tqdm import trange\n",
            "import pandas as pd\n",
            "import copy\n",
            "\n",
            "from torch_geometric.datasets import TUDataset\n",
            "from torch_geometric.datasets import Planetoid\n",
            "from torch_geometric.loader import DataLoader\n",
            "\n",
            "import torch_geometric.nn as pyg_nn\n",
            "\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "train_loader = DataLoader(MinCostDataset(root = \"./data/data_train\"), batch_size = args.batch_size, shuffle = True)\n",
            "test_loader = DataLoader(MinCostDataset(root = \"./data/data_test\"), batch_size = args.batch_size, shuffle = True)\n",
            "# TODO also define the validation loader (and set validation fraction > 0 lol)\n",
            "\n",
            "# Output dimension is 1 since we predict scalar potential values for each vertex\n",
            "model = CBN(1, 1, 2, args)\n",
            "loss_fn = DualLoss()\n",
            "scheduler, opt = build_optimizer(args, model.parameters())\n",
            "\n",
            "def train(args):\n",
            "\n",
            "    # train\n",
            "    losses = []\n",
            "    test_accs = []\n",
            "    best_acc = 0\n",
            "    best_model = None\n",
            "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
            "        total_loss = 0\n",
            "        model.train()\n",
            "\n",
            "        for batch in tqdm(train_loader):\n",
            "            print(f\"BATCH {batch}\")\n",
            "            opt.zero_grad()\n",
            "            pred = model(batch.x, batch.edge_index, batch.edge_attr)\n",
            "            print(f\"BATCH y: {batch.y.shape}\")\n",
            "            # pred = pred[batch.train_mask]\n",
            "            # label = label[batch.train_mask]\n",
            "            loss = loss_fn(pred, batch.y, batch.x, batch.edge_index, batch.edge_attr)\n",
            "            loss.backward()\n",
            "            opt.step()\n",
            "            total_loss += loss.item() * batch.num_graphs\n",
            "        total_loss /= len(train_loader.dataset)\n",
            "        losses.append(total_loss)\n",
            "\n",
            "        if epoch % 10 == 0:\n",
            "          test_acc = test(test_loader, model)\n",
            "          test_accs.append(test_acc)\n",
            "          if test_acc > best_acc:\n",
            "            best_acc = test_acc\n",
            "            best_model = copy.deepcopy(model)\n",
            "        else:\n",
            "          test_accs.append(test_accs[-1])\n",
            "\n",
            "    return test_accs, losses, best_model, best_acc\n",
            "\n",
            "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
            "    # TODO handle is_validation\n",
            "    test_model.eval()\n",
            "    total_loss = 0\n",
            "    predictions = {}\n",
            "\n",
            "    for batch in tqdm(loader):\n",
            "        with torch.no_grad():\n",
            "            pred = model(batch.x, batch.edge_index, batch.edge_attr)\n",
            "\n",
            "            loss = loss_fn(pred, batch.y, batch.x, batch.edge_index, batch.edge_attr)\n",
            "            total_loss += loss.item() * batch.num_graphs\n",
            "\n",
            "            # TODO handle save_model_preds (Q: how to keep track of which original file we're working on?) inspiration in commented code below\n",
            "\n",
            "            # if save_model_preds:\n",
            "            #     print (\"Saving Model Predictions for Model Type\", model_type)\n",
            "            #\n",
            "            #     data = {}\n",
            "            #     data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
            "            #     data['label'] = label.view(-1).cpu().detach().numpy()\n",
            "            #\n",
            "            #     df = pd.DataFrame(data=data)\n",
            "            #     # Save locally as csv\n",
            "            #     df.to_csv('MinCostFlow-' + model_type + '.csv', sep=',', index=False)\n",
            "\n",
            "    total_loss /= len(train_loader.dataset)\n",
            "\n",
            "    return total_loss"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": [
            "test_accs, losses, best_model, best_acc = train(args)\n",
            "\n",
            "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
            "print(\"Minimum loss: {0}\".format(min(losses)))\n",
            "\n",
            "# Run test for our best model to save the predictions!\n",
            "# test(test_loader, best_model, is_validation=False, save_model_preds=True, model_type=model)\n",
            "print()\n",
            "\n",
            "plt.title(dataset.name)\n",
            "plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
            "plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
            "plt.legend()\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "collapsed": false
         },
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.16"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 0
}
