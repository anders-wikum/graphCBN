{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch has version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch has version {}\".format(torch.__version__))\n",
    "import torch.nn as nn\n",
    "from MinCostDataset import MinCostDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(1)\n",
    "from tqdm import tqdm\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "dataset = MinCostDataset(root = \"./data/\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinCostDataset(140)\n",
      "num features: 1\n",
      "num edge features: 2\n",
      "first graph: Data(x=[9559, 1], edge_index=[2, 29682], edge_attr=[29682, 2], y=[1, 1], filename='road_flow_01_DC_b.txt')\n"
     ]
    }
   ],
   "source": [
    "def dataset_information(dataset):\n",
    "    print(dataset)\n",
    "    print(f\"num features: {dataset.num_features}\")\n",
    "    print(f\"num edge features: {dataset.num_edge_features}\")\n",
    "    print(f\"first graph: {dataset[0]}\")\n",
    "\n",
    "dataset_information(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def train_test_validation_split(dataset, train = 0.7, validation = 0.15):\n",
    "    \"\"\"\n",
    "    Test split is 1 - train - validation\n",
    "    \"\"\"\n",
    "\n",
    "    length = dataset.len()\n",
    "    shuffled_dataset = np.arange(length)\n",
    "    np.random.shuffle(shuffled_dataset)\n",
    "\n",
    "    train_cutoff = int(train * length)\n",
    "    validation_cutoff = int((train + validation) * length)\n",
    "\n",
    "    train_data = shuffled_dataset[:train_cutoff]\n",
    "    validation_data = shuffled_dataset[train_cutoff: validation_cutoff]\n",
    "    test_data = shuffled_dataset[validation_cutoff:]\n",
    "\n",
    "    return train_data, validation_data, test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def create_split_directories(dataset, split, split_name):\n",
    "    src_folder = dataset.processed_dir\n",
    "    dst_folder = os.path.join(dataset.root, split_name)\n",
    "\n",
    "    # Remove files in case some were already present\n",
    "    if os.path.exists(dst_folder):\n",
    "        shutil.rmtree(dst_folder)\n",
    "    os.makedirs(dst_folder)\n",
    "    dst_index = 0\n",
    "    for file_id in split:\n",
    "        src_file_name = f\"data_{file_id}.pt\"\n",
    "        # The files are always expected by PyG to be ordered\n",
    "        dst_file_name = f\"data_{dst_index}.pt\"\n",
    "        src = os.path.join(src_folder, src_file_name)\n",
    "        dst = os.path.join(dst_folder, dst_file_name)\n",
    "        shutil.copyfile(src, dst)\n",
    "        dst_index += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_frac = 0.7, validation_frac = 0.15):\n",
    "    train, validation, test = train_test_validation_split(dataset, train_frac, validation_frac)\n",
    "    create_split_directories(dataset, train, \"data_train/processed\")\n",
    "    create_split_directories(dataset, test, \"data_test/processed\")\n",
    "    create_split_directories(dataset, validation, \"data_validation/processed\")\n",
    "split_dataset(dataset, validation_frac = 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "class CBN(torch.nn.Module):\n",
    "    #TODO cite the colab\n",
    "    def __init__(self, input_dim, output_dim, edge_feature_dim, args):\n",
    "        super(CBN, self).__init__()\n",
    "\n",
    "        hidden_dim = args.hidden_dim\n",
    "        num_layers = args.num_layers\n",
    "        dropout = args.dropout\n",
    "\n",
    "        if num_layers > 1:\n",
    "            conv_modules = [NNConv(input_dim, hidden_dim, nn.Linear(edge_feature_dim, input_dim * hidden_dim))]\n",
    "            conv_modules.extend([NNConv(hidden_dim, hidden_dim, nn.Linear(edge_feature_dim, hidden_dim * hidden_dim)) for _ in range(num_layers - 2)])\n",
    "            conv_modules.append(NNConv(hidden_dim, output_dim, nn.Linear(edge_feature_dim, hidden_dim * output_dim)))\n",
    "\n",
    "            self.convs = nn.ModuleList(conv_modules)\n",
    "        else:\n",
    "            self.convs = nn.ModuleList([NNConv(input_dim, output_dim, nn.Linear(edge_feature_dim, input_dim * output_dim))])\n",
    "\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
    "\n",
    "        # self.post_mp = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Probability of an element getting zeroed\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        for bn in self.bns:\n",
    "            bn.reset_parameters()\n",
    "        # self.post_mp.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        for i in range(self.num_layers - 1):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        # x = self.post_mp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def dual_value(N, p):\n",
    "        return np.sum([p[i] * N.b[i] for i in N.V]) + np.sum([N.u[e] * max(0, p[e[1]] - p[e[0]] - N.c[e]) for e in N.E])\n",
    "\n",
    "    # def loss(self, pred, label, x, edge_index, edge_attr):\n",
    "    #     # edge_attr[0] is capacity, edge_attr[1] is cost\n",
    "    #     print(pred.shape)\n",
    "    #     print(edge_index[0].shape)\n",
    "    #     print(pred[edge_index[1]].shape)\n",
    "    #     print(edge_attr[:, 1].shape)\n",
    "    #     reduced_cost = pred[edge_index[1]].squeeze() - pred[edge_index[0]].squeeze() - edge_attr[:, 1]\n",
    "    #     print(reduced_cost.shape)\n",
    "    #     return label - torch.dot(pred.squeeze(), x.squeeze()) - torch.dot(edge_attr[:, 0], F.relu(reduced_cost))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "class DualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DualLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, label, x, edge_index, edge_attr):\n",
    "        # edge_attr[0] is capacity, edge_attr[1] is cost\n",
    "        #TODO is negative for the moment but if you switch the sign before the second dot product it's always positive ._.\n",
    "        reduced_cost = pred[edge_index[1]].squeeze() - pred[edge_index[0]].squeeze() - edge_attr[:, 1]\n",
    "        loss = (label - torch.dot(pred.squeeze(), x.squeeze()) + torch.dot(edge_attr[:, 0], F.relu(reduced_cost))) / label\n",
    "        print(f\"loss: {loss}\")\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor([[98.3923]], grad_fn=<DivBackward0>)\n",
      "loss: 98.39229583740234\n",
      "loss: tensor([[24.5549]], grad_fn=<DivBackward0>)\n",
      "loss: 24.55490493774414\n",
      "loss: tensor([[11.1987]], grad_fn=<DivBackward0>)\n",
      "loss: 11.198662757873535\n",
      "loss: tensor([[121.5326]], grad_fn=<DivBackward0>)\n",
      "loss: 121.53262329101562\n",
      "loss: tensor([[108.6385]], grad_fn=<DivBackward0>)\n",
      "loss: 108.6385269165039\n",
      "loss: tensor([[153.9402]], grad_fn=<DivBackward0>)\n",
      "loss: 153.94021606445312\n",
      "loss: tensor([[0.9769]], grad_fn=<DivBackward0>)\n",
      "loss: 0.976935625076294\n",
      "loss: tensor([[32.2227]], grad_fn=<DivBackward0>)\n",
      "loss: 32.22270584106445\n",
      "loss: tensor([[73.6299]], grad_fn=<DivBackward0>)\n",
      "loss: 73.62993621826172\n",
      "loss: tensor([[147.0968]], grad_fn=<DivBackward0>)\n",
      "loss: 147.09681701660156\n",
      "loss: tensor([[28.9313]], grad_fn=<DivBackward0>)\n",
      "loss: 28.931272506713867\n",
      "loss: tensor([[27.6645]], grad_fn=<DivBackward0>)\n",
      "loss: 27.66445541381836\n",
      "loss: tensor([[51.0615]], grad_fn=<DivBackward0>)\n",
      "loss: 51.06151580810547\n",
      "loss: tensor([[5.0610]], grad_fn=<DivBackward0>)\n",
      "loss: 5.0610270500183105\n",
      "loss: tensor([[67.2972]], grad_fn=<DivBackward0>)\n",
      "loss: 67.29716491699219\n",
      "loss: tensor([[33.8372]], grad_fn=<DivBackward0>)\n",
      "loss: 33.837215423583984\n",
      "loss: tensor([[9.3658]], grad_fn=<DivBackward0>)\n",
      "loss: 9.365843772888184\n",
      "loss: tensor([[66.0124]], grad_fn=<DivBackward0>)\n",
      "loss: 66.01238250732422\n",
      "loss: tensor([[81.8736]], grad_fn=<DivBackward0>)\n",
      "loss: 81.87360382080078\n",
      "loss: tensor([[91.9561]], grad_fn=<DivBackward0>)\n",
      "loss: 91.95610046386719\n",
      "loss: tensor([[43.1969]], grad_fn=<DivBackward0>)\n",
      "loss: 43.19688034057617\n",
      "loss: tensor([[30.9660]], grad_fn=<DivBackward0>)\n",
      "loss: 30.96596908569336\n",
      "loss: tensor([[11.6167]], grad_fn=<DivBackward0>)\n",
      "loss: 11.61669921875\n",
      "loss: tensor([[44.1581]], grad_fn=<DivBackward0>)\n",
      "loss: 44.15813064575195\n",
      "loss: tensor([[21.7938]], grad_fn=<DivBackward0>)\n",
      "loss: 21.793832778930664\n",
      "loss: tensor([[19.1313]], grad_fn=<DivBackward0>)\n",
      "loss: 19.13127899169922\n",
      "loss: tensor([[36.2761]], grad_fn=<DivBackward0>)\n",
      "loss: 36.276084899902344\n",
      "loss: tensor([[27.6863]], grad_fn=<DivBackward0>)\n",
      "loss: 27.686294555664062\n",
      "loss: tensor([[66.1172]], grad_fn=<DivBackward0>)\n",
      "loss: 66.11715698242188\n",
      "loss: tensor([[0.9982]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9982163906097412\n",
      "loss: tensor([[55.1117]], grad_fn=<DivBackward0>)\n",
      "loss: 55.111732482910156\n",
      "loss: tensor([[32.9178]], grad_fn=<DivBackward0>)\n",
      "loss: 32.917842864990234\n",
      "loss: tensor([[16.4091]], grad_fn=<DivBackward0>)\n",
      "loss: 16.40910530090332\n",
      "loss: tensor([[73.4560]], grad_fn=<DivBackward0>)\n",
      "loss: 73.45602416992188\n",
      "loss: tensor([[46.5400]], grad_fn=<DivBackward0>)\n",
      "loss: 46.54002380371094\n",
      "loss: tensor([[62.1661]], grad_fn=<DivBackward0>)\n",
      "loss: 62.16606140136719\n",
      "loss: tensor([[9.3595]], grad_fn=<DivBackward0>)\n",
      "loss: 9.359457969665527\n",
      "loss: tensor([[92.3868]], grad_fn=<DivBackward0>)\n",
      "loss: 92.38675689697266\n",
      "loss: tensor([[103.9078]], grad_fn=<DivBackward0>)\n",
      "loss: 103.90782165527344\n",
      "loss: tensor([[125.7502]], grad_fn=<DivBackward0>)\n",
      "loss: 125.75019836425781\n",
      "loss: tensor([[68.1683]], grad_fn=<DivBackward0>)\n",
      "loss: 68.1683120727539\n",
      "loss: tensor([[23.0881]], grad_fn=<DivBackward0>)\n",
      "loss: 23.088104248046875\n",
      "loss: tensor([[51.1433]], grad_fn=<DivBackward0>)\n",
      "loss: 51.14328384399414\n",
      "loss: tensor([[29.3322]], grad_fn=<DivBackward0>)\n",
      "loss: 29.33217430114746\n",
      "loss: tensor([[22.6060]], grad_fn=<DivBackward0>)\n",
      "loss: 22.60602378845215\n",
      "loss: tensor([[78.4652]], grad_fn=<DivBackward0>)\n",
      "loss: 78.46524810791016\n",
      "loss: tensor([[1.0082]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0082383155822754\n",
      "loss: tensor([[8.1341]], grad_fn=<DivBackward0>)\n",
      "loss: 8.1340970993042\n",
      "loss: tensor([[45.0579]], grad_fn=<DivBackward0>)\n",
      "loss: 45.057861328125\n",
      "loss: tensor([[68.9829]], grad_fn=<DivBackward0>)\n",
      "loss: 68.98294067382812\n",
      "loss: tensor([[0.9915]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9915294647216797\n",
      "loss: tensor([[37.5511]], grad_fn=<DivBackward0>)\n",
      "loss: 37.55107116699219\n",
      "loss: tensor([[91.4946]], grad_fn=<DivBackward0>)\n",
      "loss: 91.49459838867188\n",
      "loss: tensor([[38.5399]], grad_fn=<DivBackward0>)\n",
      "loss: 38.5399284362793\n",
      "loss: tensor([[34.2163]], grad_fn=<DivBackward0>)\n",
      "loss: 34.21625518798828\n",
      "loss: tensor([[36.9610]], grad_fn=<DivBackward0>)\n",
      "loss: 36.96100616455078\n",
      "loss: tensor([[2.7396]], grad_fn=<DivBackward0>)\n",
      "loss: 2.7395708560943604\n",
      "loss: tensor([[7.1428]], grad_fn=<DivBackward0>)\n",
      "loss: 7.142782688140869\n",
      "loss: tensor([[11.6619]], grad_fn=<DivBackward0>)\n",
      "loss: 11.66190242767334\n",
      "loss: tensor([[12.1447]], grad_fn=<DivBackward0>)\n",
      "loss: 12.144730567932129\n",
      "loss: tensor([[14.3470]], grad_fn=<DivBackward0>)\n",
      "loss: 14.34697151184082\n",
      "loss: tensor([[62.3328]], grad_fn=<DivBackward0>)\n",
      "loss: 62.33283233642578\n",
      "loss: tensor([[59.2882]], grad_fn=<DivBackward0>)\n",
      "loss: 59.28815841674805\n",
      "loss: tensor([[71.9042]], grad_fn=<DivBackward0>)\n",
      "loss: 71.90424346923828\n",
      "loss: tensor([[2.0598]], grad_fn=<DivBackward0>)\n",
      "loss: 2.059781074523926\n",
      "loss: tensor([[5.6690]], grad_fn=<DivBackward0>)\n",
      "loss: 5.669009685516357\n",
      "loss: tensor([[23.2212]], grad_fn=<DivBackward0>)\n",
      "loss: 23.22124671936035\n",
      "loss: tensor([[10.3378]], grad_fn=<DivBackward0>)\n",
      "loss: 10.337784767150879\n",
      "loss: tensor([[38.6561]], grad_fn=<DivBackward0>)\n",
      "loss: 38.6561393737793\n",
      "loss: tensor([[1.8776]], grad_fn=<DivBackward0>)\n",
      "loss: 1.8776205778121948\n",
      "loss: tensor([[15.7996]], grad_fn=<DivBackward0>)\n",
      "loss: 15.799600601196289\n",
      "loss: tensor([[91.5308]], grad_fn=<DivBackward0>)\n",
      "loss: 91.53079223632812\n",
      "loss: tensor([[0.9677]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9677338004112244\n",
      "loss: tensor([[2.0529]], grad_fn=<DivBackward0>)\n",
      "loss: 2.0528602600097656\n",
      "loss: tensor([[23.7769]], grad_fn=<DivBackward0>)\n",
      "loss: 23.776947021484375\n",
      "loss: tensor([[53.0869]], grad_fn=<DivBackward0>)\n",
      "loss: 53.08686447143555\n",
      "loss: tensor([[0.9998]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9998273253440857\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[4.6506]], grad_fn=<DivBackward0>)\n",
      "loss: 4.650576591491699\n",
      "loss: tensor([[0.9602]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9602424502372742\n",
      "loss: tensor([[0.9868]], grad_fn=<DivBackward0>)\n",
      "loss: 0.986765444278717\n",
      "loss: tensor([[1.1666]], grad_fn=<DivBackward0>)\n",
      "loss: 1.1665986776351929\n",
      "loss: tensor([[23.3933]], grad_fn=<DivBackward0>)\n",
      "loss: 23.39331817626953\n",
      "loss: tensor([[0.9986]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9985617399215698\n",
      "loss: tensor([[31.6263]], grad_fn=<DivBackward0>)\n",
      "loss: 31.62628746032715\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[15.2340]], grad_fn=<DivBackward0>)\n",
      "loss: 15.234014511108398\n",
      "loss: tensor([[6.2446]], grad_fn=<DivBackward0>)\n",
      "loss: 6.244600772857666\n",
      "loss: tensor([[0.9994]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9994295239448547\n",
      "loss: tensor([[11.3300]], grad_fn=<DivBackward0>)\n",
      "loss: 11.330015182495117\n",
      "loss: tensor([[0.9795]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9795318841934204\n",
      "loss: tensor([[5.4463]], grad_fn=<DivBackward0>)\n",
      "loss: 5.446308135986328\n",
      "loss: tensor([[0.9986]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9986122846603394\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[12.1400]], grad_fn=<DivBackward0>)\n",
      "loss: 12.140042304992676\n",
      "loss: tensor([[2.1810]], grad_fn=<DivBackward0>)\n",
      "loss: 2.181035041809082\n",
      "loss: tensor([[1.2325]], grad_fn=<DivBackward0>)\n",
      "loss: 1.2325222492218018\n",
      "loss: tensor([[16.0893]], grad_fn=<DivBackward0>)\n",
      "loss: 16.089269638061523\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9895]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9895091652870178\n",
      "loss: tensor([[71.5607]], grad_fn=<DivBackward0>)\n",
      "loss: 71.56068420410156\n",
      "loss: tensor([[10.1467]], grad_fn=<DivBackward0>)\n",
      "loss: 10.146738052368164\n",
      "loss: tensor([[9.6833]], grad_fn=<DivBackward0>)\n",
      "loss: 9.683333396911621\n",
      "loss: tensor([[0.9980]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9980008602142334\n",
      "loss: tensor([[2.6757]], grad_fn=<DivBackward0>)\n",
      "loss: 2.6756503582000732\n",
      "loss: tensor([[30.8722]], grad_fn=<DivBackward0>)\n",
      "loss: 30.87218475341797\n",
      "loss: tensor([[0.9846]], grad_fn=<DivBackward0>)\n",
      "loss: 0.984614908695221\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9979]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9978775978088379\n",
      "loss: tensor([[3.6220]], grad_fn=<DivBackward0>)\n",
      "loss: 3.622035026550293\n",
      "loss: tensor([[0.9810]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9810261130332947\n",
      "loss: tensor([[1.3692]], grad_fn=<DivBackward0>)\n",
      "loss: 1.369232416152954\n",
      "loss: tensor([[0.9851]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9851348400115967\n",
      "loss: tensor([[9.8946]], grad_fn=<DivBackward0>)\n",
      "loss: 9.894623756408691\n",
      "loss: tensor([[0.9882]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9882051944732666\n",
      "loss: tensor([[0.9824]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9824361801147461\n",
      "loss: tensor([[0.9945]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9944779872894287\n",
      "loss: tensor([[2.8680]], grad_fn=<DivBackward0>)\n",
      "loss: 2.867999315261841\n",
      "loss: tensor([[1.7142]], grad_fn=<DivBackward0>)\n",
      "loss: 1.7142019271850586\n",
      "loss: tensor([[0.9966]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9965970516204834\n",
      "loss: tensor([[0.9931]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9930527806282043\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[47.6505]], grad_fn=<DivBackward0>)\n",
      "loss: 47.65047836303711\n",
      "loss: tensor([[0.9947]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9947128295898438\n",
      "loss: tensor([[7.5013]], grad_fn=<DivBackward0>)\n",
      "loss: 7.50129508972168\n",
      "loss: tensor([[0.9900]], grad_fn=<DivBackward0>)\n",
      "loss: 0.990028977394104\n",
      "loss: tensor([[0.9929]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9929405450820923\n",
      "loss: tensor([[18.0834]], grad_fn=<DivBackward0>)\n",
      "loss: 18.083351135253906\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[21.5928]], grad_fn=<DivBackward0>)\n",
      "loss: 21.592792510986328\n",
      "loss: tensor([[32.1252]], grad_fn=<DivBackward0>)\n",
      "loss: 32.12519073486328\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[7.8587]], grad_fn=<DivBackward0>)\n",
      "loss: 7.8586530685424805\n",
      "loss: tensor([[41.2038]], grad_fn=<DivBackward0>)\n",
      "loss: 41.20384216308594\n",
      "loss: tensor([[10.1236]], grad_fn=<DivBackward0>)\n",
      "loss: 10.123632431030273\n",
      "loss: tensor([[1.0981]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0980514287948608\n",
      "loss: tensor([[1.9222]], grad_fn=<DivBackward0>)\n",
      "loss: 1.922200083732605\n",
      "loss: tensor([[1.9907]], grad_fn=<DivBackward0>)\n",
      "loss: 1.990678310394287\n",
      "loss: tensor([[0.9985]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9985015988349915\n",
      "loss: tensor([[0.9983]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9982872605323792\n",
      "loss: tensor([[3.9429]], grad_fn=<DivBackward0>)\n",
      "loss: 3.9428999423980713\n",
      "loss: tensor([[5.5985]], grad_fn=<DivBackward0>)\n",
      "loss: 5.598548412322998\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9917]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9917416572570801\n",
      "loss: tensor([[20.9725]], grad_fn=<DivBackward0>)\n",
      "loss: 20.972476959228516\n",
      "loss: tensor([[0.9997]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9996930956840515\n",
      "loss: tensor([[2.4168]], grad_fn=<DivBackward0>)\n",
      "loss: 2.4168014526367188\n",
      "loss: tensor([[13.5218]], grad_fn=<DivBackward0>)\n",
      "loss: 13.521838188171387\n",
      "loss: tensor([[27.7585]], grad_fn=<DivBackward0>)\n",
      "loss: 27.758514404296875\n",
      "loss: tensor([[1.0954]], grad_fn=<DivBackward0>)\n",
      "loss: 1.095357894897461\n",
      "loss: tensor([[1.0262]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0261738300323486\n",
      "loss: tensor([[0.9996]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9996067881584167\n",
      "loss: tensor([[39.6633]], grad_fn=<DivBackward0>)\n",
      "loss: 39.663307189941406\n",
      "loss: tensor([[33.6285]], grad_fn=<DivBackward0>)\n",
      "loss: 33.628456115722656\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[14.3841]], grad_fn=<DivBackward0>)\n",
      "loss: 14.384064674377441\n",
      "loss: tensor([[0.9873]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9873487949371338\n",
      "loss: tensor([[11.6399]], grad_fn=<DivBackward0>)\n",
      "loss: 11.639892578125\n",
      "loss: tensor([[34.7088]], grad_fn=<DivBackward0>)\n",
      "loss: 34.70878982543945\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9772]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9772025942802429\n",
      "loss: tensor([[0.9969]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9968716502189636\n",
      "loss: tensor([[0.9977]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9977224469184875\n",
      "loss: tensor([[0.9941]], grad_fn=<DivBackward0>)\n",
      "loss: 0.994147539138794\n",
      "loss: tensor([[0.9946]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9946486949920654\n",
      "loss: tensor([[39.4104]], grad_fn=<DivBackward0>)\n",
      "loss: 39.41035461425781\n",
      "loss: tensor([[2.6218]], grad_fn=<DivBackward0>)\n",
      "loss: 2.621786594390869\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[20.2060]], grad_fn=<DivBackward0>)\n",
      "loss: 20.20600700378418\n",
      "loss: tensor([[5.6561]], grad_fn=<DivBackward0>)\n",
      "loss: 5.656136989593506\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[16.4695]], grad_fn=<DivBackward0>)\n",
      "loss: 16.46953010559082\n",
      "loss: tensor([[0.9903]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9902685880661011\n",
      "loss: tensor([[0.9947]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9946873784065247\n",
      "loss: tensor([[13.1781]], grad_fn=<DivBackward0>)\n",
      "loss: 13.178102493286133\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9937]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9937145113945007\n",
      "loss: tensor([[1.8200]], grad_fn=<DivBackward0>)\n",
      "loss: 1.819993019104004\n",
      "loss: tensor([[1.3580]], grad_fn=<DivBackward0>)\n",
      "loss: 1.3579895496368408\n",
      "loss: tensor([[0.9977]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9977030158042908\n",
      "loss: tensor([[0.9995]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9994668364524841\n",
      "loss: tensor([[0.9970]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9969973564147949\n",
      "loss: tensor([[0.9985]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9984689354896545\n",
      "loss: tensor([[0.9657]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9656894207000732\n",
      "loss: tensor([[0.9997]], grad_fn=<DivBackward0>)\n",
      "loss: 0.999657392501831\n",
      "loss: tensor([[0.9969]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9969058632850647\n",
      "loss: tensor([[0.9964]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9964061379432678\n",
      "loss: tensor([[6.0304]], grad_fn=<DivBackward0>)\n",
      "loss: 6.030429840087891\n",
      "loss: tensor([[0.9812]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9812368750572205\n",
      "loss: tensor([[7.2388]], grad_fn=<DivBackward0>)\n",
      "loss: 7.238768577575684\n",
      "loss: tensor([[1.0343]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0342555046081543\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9981]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9981038570404053\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9976]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9975707530975342\n",
      "loss: tensor([[10.6092]], grad_fn=<DivBackward0>)\n",
      "loss: 10.60918140411377\n",
      "loss: tensor([[1.6256]], grad_fn=<DivBackward0>)\n",
      "loss: 1.6256033182144165\n",
      "loss: tensor([[1.6154]], grad_fn=<DivBackward0>)\n",
      "loss: 1.6153616905212402\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9931]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9931370615959167\n",
      "loss: tensor([[12.1103]], grad_fn=<DivBackward0>)\n",
      "loss: 12.110288619995117\n",
      "loss: tensor([[0.9928]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9927651882171631\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[9.1011]], grad_fn=<DivBackward0>)\n",
      "loss: 9.10114860534668\n",
      "loss: tensor([[28.3126]], grad_fn=<DivBackward0>)\n",
      "loss: 28.31256675720215\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9978]], grad_fn=<DivBackward0>)\n",
      "loss: 0.997817873954773\n",
      "loss: tensor([[19.4488]], grad_fn=<DivBackward0>)\n",
      "loss: 19.44880485534668\n",
      "loss: tensor([[1.0575]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0574843883514404\n",
      "loss: tensor([[0.9965]], grad_fn=<DivBackward0>)\n",
      "loss: 0.996515691280365\n",
      "loss: tensor([[13.2306]], grad_fn=<DivBackward0>)\n",
      "loss: 13.230570793151855\n",
      "loss: tensor([[3.5614]], grad_fn=<DivBackward0>)\n",
      "loss: 3.561380624771118\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9824]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9823721647262573\n",
      "loss: tensor([[0.9964]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9963945746421814\n",
      "loss: tensor([[0.9891]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9890697002410889\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[4.0727]], grad_fn=<DivBackward0>)\n",
      "loss: 4.072652816772461\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[32.3082]], grad_fn=<DivBackward0>)\n",
      "loss: 32.30821228027344\n",
      "loss: tensor([[13.9941]], grad_fn=<DivBackward0>)\n",
      "loss: 13.994115829467773\n",
      "loss: tensor([[0.9761]], grad_fn=<DivBackward0>)\n",
      "loss: 0.976146936416626\n",
      "loss: tensor([[19.6893]], grad_fn=<DivBackward0>)\n",
      "loss: 19.68926239013672\n",
      "loss: tensor([[0.9991]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9991195201873779\n",
      "loss: tensor([[0.9989]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9989219903945923\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[1.3761]], grad_fn=<DivBackward0>)\n",
      "loss: 1.3761132955551147\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9795]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9795343279838562\n",
      "loss: tensor([[0.9640]], grad_fn=<DivBackward0>)\n",
      "loss: 0.963964581489563\n",
      "loss: tensor([[0.9994]], grad_fn=<DivBackward0>)\n",
      "loss: 0.999436616897583\n",
      "loss: tensor([[0.9905]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9905485510826111\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[0.9948]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9948427081108093\n",
      "loss: tensor([[0.9970]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9969701170921326\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[1.]], grad_fn=<DivBackward0>)\n",
      "loss: 1.0\n",
      "loss: tensor([[1.0469]], grad_fn=<DivBackward0>)\n",
      "loss: 1.046893835067749\n",
      "loss: tensor([[1.2510]], grad_fn=<DivBackward0>)\n",
      "loss: 1.2510347366333008\n",
      "loss: tensor([[0.9992]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9992001056671143\n",
      "loss: tensor([[0.9961]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9960522651672363\n",
      "loss: tensor([[0.9868]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9867924451828003\n",
      "loss: tensor([[0.9943]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9942936897277832\n",
      "loss: tensor([[1.2816]], grad_fn=<DivBackward0>)\n",
      "loss: 1.2816051244735718\n",
      "loss: tensor([[0.9995]], grad_fn=<DivBackward0>)\n",
      "loss: 0.9994903802871704\n",
      "loss: tensor([[2.2968]], grad_fn=<DivBackward0>)\n",
      "loss: 2.2968459129333496\n",
      "tensor([[ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [43.3844],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#TODO handle batch size > 1\n",
    "args = {\n",
    "    'num_layers': 5,\n",
    "    'batch_size': 1,\n",
    "    'hidden_dim': 64,\n",
    "    'dropout': 0.5,\n",
    "    'epochs': 500,\n",
    "    'opt': 'adam',\n",
    "    'opt_scheduler': 'none',\n",
    "    'opt_restart': 0,\n",
    "    'weight_decay': 5e-3,\n",
    "    'lr': 0.001\n",
    "}\n",
    "args = objectview(args)\n",
    "model = CBN(1, 1, 2, args)\n",
    "loss_fn = DualLoss()\n",
    "data = dataset[40]\n",
    "\n",
    "\n",
    "scheduler, opt = build_optimizer(args, model.parameters())\n",
    "for i in range(250):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    pred = model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = loss_fn(pred, data.y, data.x, data.edge_index, data.edge_attr)\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "print(pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n        0.      ,  0.      ,  0.      , 43.384396,  0.      ,  0.      ,\n        0.      ,  0.      ,  0.      ,  0.      ,  0.      ,  0.      ,\n        0.      ,  0.      ], dtype=float32)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pred.detach().numpy().flatten()\n",
    "p"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_loader = DataLoader(MinCostDataset(root = \"./data/data_train\"), batch_size = args.batch_size, shuffle = True)\n",
    "test_loader = DataLoader(MinCostDataset(root = \"./data/data_test\"), batch_size = args.batch_size, shuffle = True)\n",
    "# TODO also define the validation loader (and set validation fraction > 0 lol)\n",
    "\n",
    "# Output dimension is 1 since we predict scalar potential values for each vertex\n",
    "model = CBN(1, 1, 2, args)\n",
    "loss_fn = DualLoss()\n",
    "scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            print(f\"BATCH {batch}\")\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            print(f\"BATCH y: {batch.y.shape}\")\n",
    "            # pred = pred[batch.train_mask]\n",
    "            # label = label[batch.train_mask]\n",
    "            loss = loss_fn(pred, batch.y, batch.x, batch.edge_index, batch.edge_attr)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "\n",
    "    return test_accs, losses, best_model, best_acc\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
    "    # TODO handle is_validation\n",
    "    test_model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = {}\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "\n",
    "            loss = loss_fn(pred, batch.y, batch.x, batch.edge_index, batch.edge_attr)\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "            # TODO handle save_model_preds (Q: how to keep track of which original file we're working on?) inspiration in commented code below\n",
    "\n",
    "            # if save_model_preds:\n",
    "            #     print (\"Saving Model Predictions for Model Type\", model_type)\n",
    "            #\n",
    "            #     data = {}\n",
    "            #     data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "            #     data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "            #\n",
    "            #     df = pd.DataFrame(data=data)\n",
    "            #     # Save locally as csv\n",
    "            #     df.to_csv('MinCostFlow-' + model_type + '.csv', sep=',', index=False)\n",
    "\n",
    "    total_loss /= len(train_loader.dataset)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/500 [00:00<?, ?Epochs/s]\n",
      "  0%|          | 0/98 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH DataBatch(x=[9559, 1], edge_index=[2, 29682], edge_attr=[29682, 2], y=[1, 1], filename=[1], batch=[9559], ptr=[2])\n",
      "BATCH y: torch.Size([1, 1])\n",
      "loss: tensor([[35.2059]], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/98 [00:01<02:49,  1.75s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH DataBatch(x=[20, 1], edge_index=[2, 66], edge_attr=[66, 2], y=[1, 1], filename=[1], batch=[20], ptr=[2])\n",
      "BATCH y: torch.Size([1, 1])\n",
      "loss: tensor([[123.3727]], grad_fn=<DivBackward0>)\n",
      "BATCH DataBatch(x=[20, 1], edge_index=[2, 66], edge_attr=[66, 2], y=[1, 1], filename=[1], batch=[20], ptr=[2])\n",
      "BATCH y: torch.Size([1, 1])\n",
      "loss: tensor([[29.3609]], grad_fn=<DivBackward0>)\n",
      "BATCH DataBatch(x=[116915, 1], edge_index=[2, 262958], edge_attr=[262958, 2], y=[1, 1], filename=[1], batch=[116915], ptr=[2])\n",
      "BATCH y: torch.Size([1, 1])\n",
      "loss: tensor([[-8.0961e+09]], grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/98 [00:55<29:28, 18.61s/it]\n",
      "Training:   0%|          | 0/500 [00:55<?, ?Epochs/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test_accs, losses, best_model, best_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMaximum test set accuracy: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mmax\u001B[39m(test_accs)))\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMinimum loss: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mmin\u001B[39m(losses)))\n",
      "Cell \u001B[0;32mIn[45], line 47\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(args)\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# pred = pred[batch.train_mask]\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# label = label[batch.train_mask]\u001B[39;00m\n\u001B[1;32m     46\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(pred, batch\u001B[38;5;241m.\u001B[39my, batch\u001B[38;5;241m.\u001B[39mx, batch\u001B[38;5;241m.\u001B[39medge_index, batch\u001B[38;5;241m.\u001B[39medge_attr)\n\u001B[0;32m---> 47\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     49\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m batch\u001B[38;5;241m.\u001B[39mnum_graphs\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/torch/_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    487\u001B[0m     )\n\u001B[0;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/cs224w/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test_accs, losses, best_model, best_acc = train(args)\n",
    "\n",
    "print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "# Run test for our best model to save the predictions!\n",
    "# test(test_loader, best_model, is_validation=False, save_model_preds=True, model_type=model)\n",
    "print()\n",
    "\n",
    "plt.title(dataset.name)\n",
    "plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
    "plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
